{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline - Understanding Batch Inference for Paper Evaluation\n",
    "\n",
    "In this workshop, we'll build a simple paper evaluation system that:\n",
    "1. **Fetches** recent AI research papers from arXiv\n",
    "2. **Evaluates** each paper's relevance using the Batch API\n",
    "3. **Ranks** papers to find the most interesting ones for our team\n",
    "\n",
    "This is our **baseline** - a fixed pipeline with no decision-making.\n",
    "In later parts, we'll transform this pipeline to use additional tools and do more intelligence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import our libraries and configure the environment.\n",
    "\n",
    "**What we're using:**\n",
    "- `arxiv` - Python client for searching arXiv papers\n",
    "- `openai` - Client for the OpenAI-compatible Batch API. We will be using the Doubleword Batch API in our example.\n",
    "- `dotenv` - Load API keys from `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import arxiv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add src directory to path so we can import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our Doubleword API Key\n",
    "\n",
    "Now we need to create a Doubleword API Key. \n",
    "\n",
    "1. Sign up at https://app.doubleword.com\n",
    "2. Create an API key, save this in a safe place\n",
    "3. Copy the `.env.example` file in the same directory, name it `.env` and add your api key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API key loaded\n",
      "âœ“ Model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "âœ“ Base URL: https://api.doubleword.ai/v1\n"
     ]
    }
   ],
   "source": [
    "# Configuration - these come from your .env file\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")  # Optional: for non-OpenAI providers\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Verify we have an API key\n",
    "if not API_KEY:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found in environment!\")\n",
    "    print(\"   Create a .env file with your API key, or we'll use sample data.\")\n",
    "else:\n",
    "    print(f\"âœ“ API key loaded\")\n",
    "    print(f\"âœ“ Model: {MODEL_NAME}\")\n",
    "    if BASE_URL:\n",
    "        print(f\"âœ“ Base URL: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a quick test to make sure that we have established connectivity to Doubleword. The Doubleword follows the openai spec, so let's see which models we have availabile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  â€¢ Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "  â€¢ Qwen/Qwen3-Embedding-8B\n",
      "  â€¢ Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model in client.models.list():\n",
    "    print(f\"  â€¢ {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We're Building\n",
    "\n",
    "Our goal is to help an AI engineering team stay current with research.\n",
    "Every day, hundreds of papers are published - we can't read them all!\n",
    "\n",
    "**The Pipeline:**\n",
    "```\n",
    "arXiv API â†’ Fetch Papers â†’ Batch API â†’ Evaluate Relevance â†’ Ranked List to Return to User\n",
    "```\n",
    "\n",
    "**Why Batch API?**\n",
    "- 50% cheaper than real-time API calls\n",
    "- Process many papers in parallel\n",
    "- Perfect for non-interactive workloads\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch Papers from arXiv\n",
    "\n",
    "arXiv is a free repository of research papers, widely used in AI/ML.\n",
    "We'll search for recent papers matching our keywords.\n",
    "\n",
    "Let's look at the code that fetches papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers(keywords: list[str], max_results: int = 20, days_back: int = 7) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch recent papers from arXiv matching the given keywords.\n",
    "    \n",
    "    Args:\n",
    "        keywords: Search terms (e.g., [\"LLM\", \"transformers\"])\n",
    "        max_results: Maximum papers to return\n",
    "        days_back: How far back to search\n",
    "    \n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate cutoff date\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    \n",
    "    # Build search query: \"keyword1\" OR \"keyword2\" OR ...\n",
    "    # Quotes ensure exact phrase matching\n",
    "    query = \" OR \".join([f'\"{kw}\"' for kw in keywords])\n",
    "    print(f\"ðŸ” Searching arXiv for: {query}\")\n",
    "    print(f\"ðŸ“… Looking for papers since: {cutoff_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Configure the arXiv search\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,  # Most recent first\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Execute search and collect results\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for paper in client.results(search):\n",
    "        # Only include papers from our time window\n",
    "        paper_date = paper.published.replace(tzinfo=None)\n",
    "        if paper_date >= cutoff_date:\n",
    "            papers.append({\n",
    "                'id': paper.entry_id.split('/')[-1],  # e.g., \"2401.12345v1\"\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'abstract': paper.summary,\n",
    "                'published': paper.published.isoformat(),\n",
    "                'url': paper.entry_id\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ“ Found {len(papers)} papers\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Fetch Live Papers from arXiv\n",
    "\n",
    "This searches arXiv in real-time. Results vary based on what's been published recently. You can tweak `max_results` to get more or less papers to search from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_papers import get_papers\n",
    "\n",
    "# Define what we're interested in\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"large language models\",\n",
    "    \"LLM agents\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "# Fetch papers (set to False to use sample data instead)\n",
    "USE_LIVE_DATA = True  # Change to True to fetch from arXiv\n",
    "\n",
    "if USE_LIVE_DATA:\n",
    "    papers = get_papers(SEARCH_KEYWORDS, max_results=20)\n",
    "else:\n",
    "    print(\"ðŸ“‚ Using sample data (set USE_LIVE_DATA = True to fetch live)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load Sample Papers\n",
    "For consistent results during the workshop, we'll use pre-saved sample papers.\n",
    "This also works offline and doesn't require waiting for arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_papers(filepath: str = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load pre-saved sample papers for offline demos.\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        # Default path relative to notebooks directory\n",
    "        filepath = Path.cwd().parent / \"sample_data\" / \"papers.json\"\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        papers = json.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(papers)} sample papers\")\n",
    "    return papers\n",
    "\n",
    "# Load sample papers if we didn't fetch live ones\n",
    "if not USE_LIVE_DATA:\n",
    "    papers = load_sample_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Papers\n",
    "\n",
    "Let's see what we got back from arXiv (or our sample data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 20\n",
      "\n",
      "ðŸ“„ Structure of a paper object:\n",
      "   Keys: ['id', 'title', 'authors', 'abstract', 'published', 'url']\n"
     ]
    }
   ],
   "source": [
    "# How many papers do we have?\n",
    "print(f\"Total papers: {len(papers)}\")\n",
    "print()\n",
    "\n",
    "# Look at the structure of one paper\n",
    "print(\"ðŸ“„ Structure of a paper object:\")\n",
    "print(f\"   Keys: {list(papers[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIRST 5 PAPERS\n",
      "================================================================================\n",
      "\n",
      "1. Understanding and Exploiting Weight Update Sparsity for Communication-...\n",
      "   ID: 2602.03839v1\n",
      "   Authors: Erfan Miahi, Eugene Belilovsky\n",
      "   Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, ...\n",
      "\n",
      "2. Accelerating Scientific Research with Gemini: Case Studies and Common ...\n",
      "   ID: 2602.03837v1\n",
      "   Authors: David P. Woodruff, Vincent Cohen-Addad, Lalit Jain...\n",
      "   Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of...\n",
      "\n",
      "3. Fast-Slow Efficient Training for Multimodal Large Language Models via ...\n",
      "   ID: 2602.03815v1\n",
      "   Authors: Dingkun Zhang, Shuhan Qi, Yulin Wu...\n",
      "   Abstract: Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual...\n",
      "\n",
      "4. Conformal Thinking: Risk Control for Reasoning on a Compute Budget\n",
      "   ID: 2602.03814v1\n",
      "   Authors: Xi Wang, Anushri Suresh, Alvin Zhang...\n",
      "   Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adapt...\n",
      "\n",
      "5. Antidistillation Fingerprinting\n",
      "   ID: 2602.03812v1\n",
      "   Authors: Yixuan Even Xu, John Kirchenbauer, Yash Savani...\n",
      "   Abstract: Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-...\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 papers\n",
    "print(\"=\"*80)\n",
    "print(\"FIRST 5 PAPERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, paper in enumerate(papers[:5], 1):\n",
    "    print(f\"\\n{i}. {paper['title'][:70]}...\" if len(paper['title']) > 70 else f\"\\n{i}. {paper['title']}\")\n",
    "    print(f\"   ID: {paper['id']}\")\n",
    "    print(f\"   Authors: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}\")\n",
    "    print(f\"   Abstract: {paper['abstract'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Got Back\n",
    "\n",
    "Each paper is a dictionary with:\n",
    "- **id**: arXiv identifier (useful for linking back to papers)\n",
    "- **title**: The paper's title\n",
    "- **authors**: List of author names\n",
    "- **abstract**: Summary of the paper (this is what we'll evaluate!)\n",
    "- **published**: When it was submitted to arXiv\n",
    "- **url**: Direct link to the paper\n",
    "\n",
    "Now the question is: **which of these 20 papers should our team actually read?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Batch Evaluation with AI\n",
    "\n",
    "Now we'll use an LLM to evaluate each paper's relevance to our team.\n",
    "\n",
    "### The Team Profile\n",
    "\n",
    "First, we define what our team cares about. This is the \"context\" the AI uses to judge relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Focus:\n",
      "The team is building AI-powered applications and wants to stay\n",
      "        current with the latest research on language models, inference\n",
      "        optimization, and practical AI engineering.\n",
      "\n",
      "Interests:\n",
      "  â€¢ Large language model architectures and improvements\n",
      "  â€¢ Inference optimization and cost reduction\n",
      "  â€¢ Prompt engineering and techniques\n",
      "  â€¢ AI agents and tool use\n",
      "  â€¢ Evaluation methods for LLMs\n"
     ]
    }
   ],
   "source": [
    "# Define what makes a paper \"relevant\" for our team\n",
    "# Try modifying this to see how it changes the results!\n",
    "\n",
    "TEAM_PROFILE = {\n",
    "    \"focus\": \"\"\"\n",
    "        The team is building AI-powered applications and wants to stay\n",
    "        current with the latest research on language models, inference\n",
    "        optimization, and practical AI engineering.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"interests\": [\n",
    "        \"Large language model architectures and improvements\",\n",
    "        \"Inference optimization and cost reduction\",\n",
    "        \"Prompt engineering and techniques\",\n",
    "        \"AI agents and tool use\",\n",
    "        \"Evaluation methods for LLMs\",\n",
    "    ],\n",
    "    \n",
    "    \"avoid\": [\n",
    "        \"Pure theoretical papers without practical applications\",\n",
    "        \"Incremental benchmark improvements\",\n",
    "        \"Papers focused only on training from scratch\",\n",
    "        \"Papers focused on training models\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Team Focus:\")\n",
    "print(TEAM_PROFILE['focus'].strip())\n",
    "print(\"\\nInterests:\")\n",
    "for interest in TEAM_PROFILE['interests']:\n",
    "    print(f\"  â€¢ {interest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evaluation Prompt\n",
    "\n",
    "This is the prompt we send to the LLM for each paper.\n",
    "Notice how we:\n",
    "1. Give context about the team\n",
    "2. Provide the paper details\n",
    "3. Ask for structured JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ SAMPLE PROMPT (first paper):\n",
      "================================================================================\n",
      "You are evaluating research papers for an AI engineering team.\n",
      "\n",
      "TEAM PROFILE:\n",
      "The team is building AI-powered applications and wants to stay\n",
      "        current with the latest research on language models, inference\n",
      "        optimization, and practical AI engineering.\n",
      "\n",
      "What they find valuable:\n",
      "  - Large language model architectures and improvements\n",
      "  - Inference optimization and cost reduction\n",
      "  - Prompt engineering and techniques\n",
      "  - AI agents and tool use\n",
      "  - Evaluation methods for LLMs\n",
      "\n",
      "What to avoid recommending:\n",
      "  - Pure theoretical papers without practical applications\n",
      "  - Incremental benchmark improvements\n",
      "  - Papers focused only on training from scratch\n",
      "  - Papers focused on training models\n",
      "\n",
      "---\n",
      "\n",
      "PAPER TO EVALUATE:\n",
      "\n",
      "Title: Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL\n",
      "\n",
      "Abstract:\n",
      "Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamic...\n"
     ]
    }
   ],
   "source": [
    "def create_evaluation_prompt(paper: dict, team_profile: dict) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt that asks the AI to evaluate a paper.\n",
    "    \n",
    "    This is where the \"magic\" happens - we give the model:\n",
    "    1. Context about what the team cares about\n",
    "    2. The paper's title and abstract\n",
    "    3. Clear instructions on how to respond\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format interests and avoid lists as bullet points\n",
    "    interests_text = \"\\n\".join(f\"  - {item}\" for item in team_profile['interests'])\n",
    "    avoid_text = \"\\n\".join(f\"  - {item}\" for item in team_profile['avoid'])\n",
    "    \n",
    "    prompt = f\"\"\"You are evaluating research papers for an AI engineering team.\n",
    "\n",
    "TEAM PROFILE:\n",
    "{team_profile['focus'].strip()}\n",
    "\n",
    "What they find valuable:\n",
    "{interests_text}\n",
    "\n",
    "What to avoid recommending:\n",
    "{avoid_text}\n",
    "\n",
    "---\n",
    "\n",
    "PAPER TO EVALUATE:\n",
    "\n",
    "Title: {paper['title']}\n",
    "\n",
    "Abstract:\n",
    "{paper['abstract']}\n",
    "\n",
    "---\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Score this paper's relevance to the team on a scale of 0-10.\n",
    "- 0-3: Not relevant\n",
    "- 4-6: Somewhat relevant\n",
    "- 7-10: Highly relevant, team should read this\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{{\n",
    "    \"relevance_score\": <integer 0-10>,\n",
    "    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n",
    "    \"why_relevant\": \"<one sentence explaining why this score>\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Let's see what a prompt looks like\n",
    "sample_prompt = create_evaluation_prompt(papers[0], TEAM_PROFILE)\n",
    "print(\"ðŸ“ SAMPLE PROMPT (first paper):\")\n",
    "print(\"=\"*80)\n",
    "print(sample_prompt[:1500] + \"...\" if len(sample_prompt) > 1500 else sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Batch File\n",
    "\n",
    "The Batch API expects a JSONL file (one JSON object per line).\n",
    "Each line represents one request to the API. We can either build the JSONl file using `create_batch_file` below or we can use [autobatcher](https://docs.doubleword.ai/batches/autobatcher)\n",
    "\n",
    "#### Option A: Create Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created batch file: batch_input.jsonl\n",
      "  Contains 20 requests\n"
     ]
    }
   ],
   "source": [
    "def create_batch_file(papers: list[dict], model: str, output_path: str = \"batch_input.jsonl\") -> str:\n",
    "    \"\"\"\n",
    "    Create a JSONL file for the Batch API.\n",
    "    \n",
    "    Each line contains:\n",
    "    - custom_id: Links response back to the paper\n",
    "    - method: HTTP method (POST)\n",
    "    - url: API endpoint\n",
    "    - body: The actual request (model, messages, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for paper in papers:\n",
    "            request = {\n",
    "                \"custom_id\": paper['id'],  # We'll use this to match responses\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": create_evaluation_prompt(paper, TEAM_PROFILE)\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create the batch file\n",
    "batch_file = create_batch_file(papers, MODEL_NAME)\n",
    "print(f\"âœ“ Created batch file: {batch_file}\")\n",
    "print(f\"  Contains {len(papers)} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ First 2 lines of batch file:\n",
      "================================================================================\n",
      "\n",
      "Request 1:\n",
      "  custom_id: 2602.03839v1\n",
      "  method: POST\n",
      "  model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "  message length: 2844 chars\n",
      "\n",
      "Request 2:\n",
      "  custom_id: 2602.03837v1\n",
      "  method: POST\n",
      "  model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "  message length: 2713 chars\n"
     ]
    }
   ],
   "source": [
    "# Let's peek at the batch file\n",
    "print(\"ðŸ“„ First 2 lines of batch file:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open(batch_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 2:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # Show a truncated version\n",
    "        print(f\"\\nRequest {i+1}:\")\n",
    "        print(f\"  custom_id: {data['custom_id']}\")\n",
    "        print(f\"  method: {data['method']}\")\n",
    "        print(f\"  model: {data['body']['model']}\")\n",
    "        print(f\"  message length: {len(data['body']['messages'][0]['content'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to the Batch API\n",
    "\n",
    "Check out the official documenation [here](https://docs.doubleword.ai/batches/getting-started-with-batched-api) on how to submit your first batch.\n",
    "\n",
    "Now we:\n",
    "1. Upload the batch file\n",
    "2. Create a batch job\n",
    "3. Wait for completion\n",
    "4. Download results\n",
    "\n",
    "**Note:** This requires a valid API key. If you don't have one, skip to the \"Mock Results\" section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_batch(client: OpenAI, input_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload file and create a batch job.\n",
    "    Returns the batch ID for tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Upload the file\n",
    "    print(\"ðŸ“¤ Uploading batch file...\")\n",
    "    with open(input_file, 'rb') as f:\n",
    "        uploaded_file = client.files.create(file=f, purpose=\"batch\")\n",
    "    print(f\"   File ID: {uploaded_file.id}\")\n",
    "    \n",
    "    # Step 2: Create the batch job\n",
    "    print(\"ðŸš€ Creating batch job...\")\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=uploaded_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"1h\" # this can be 24hr or 1hr \n",
    "    )\n",
    "    print(f\"   Batch ID: {batch.id}\")\n",
    "    print(f\"   Status: {batch.status}\")\n",
    "    \n",
    "    return batch.id\n",
    "\n",
    "\n",
    "def wait_for_batch(client: OpenAI, batch_id: str, poll_interval: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Poll until batch completes, then return output file ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nâ³ Waiting for batch to complete...\")\n",
    "    print(\"   (This typically takes 1-5 minutes)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        status = batch.status\n",
    "        \n",
    "        # Show progress\n",
    "        completed = (batch.request_counts.completed or 0)\n",
    "        failed = (batch.request_counts.failed or 0)\n",
    "        total = batch.request_counts.total or 0\n",
    "        \n",
    "        print(f\"   Status: {status} | Progress: {completed + failed}/{total}\")\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            print(\"\\nâœ“ Batch completed!\")\n",
    "            return batch.output_file_id\n",
    "        elif status in [\"failed\", \"expired\", \"cancelled\"]:\n",
    "            raise Exception(f\"Batch failed with status: {status}\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def download_results(client: OpenAI, output_file_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Download and parse the batch results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ“¥ Downloading results...\")\n",
    "    content = client.files.content(output_file_id)\n",
    "    \n",
    "    results = []\n",
    "    for line in content.text.strip().split('\\n'):\n",
    "        if line:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    print(f\"   Downloaded {len(results)} results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Uploading batch file...\n",
      "   File ID: b2a0219c-7d25-4e09-b12f-bbc8284f846a\n",
      "ðŸš€ Creating batch job...\n",
      "   Batch ID: 0034c061-db71-418a-92d5-81bef945a90c\n",
      "   Status: in_progress\n",
      "\n",
      "â³ Waiting for batch to complete...\n",
      "   (This typically takes 1-5 minutes)\n",
      "\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: completed | Progress: 20/20\n",
      "\n",
      "âœ“ Batch completed!\n",
      "ðŸ“¥ Downloading results...\n",
      "   Downloaded 20 results\n",
      "\n",
      "âœ“ Batch evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run the batch evaluation\n",
    "# Set this to True when you have a valid API key\n",
    "RUN_BATCH_API = True\n",
    "\n",
    "if RUN_BATCH_API and API_KEY:\n",
    "    # Create the client\n",
    "    client_kwargs = {'api_key': API_KEY}\n",
    "    if BASE_URL:\n",
    "        client_kwargs['base_url'] = BASE_URL\n",
    "    \n",
    "    client = OpenAI(**client_kwargs)\n",
    "       \n",
    "    # Submit and wait\n",
    "    batch_id = submit_batch(client, batch_file)\n",
    "    output_file_id = wait_for_batch(client, batch_id)\n",
    "    raw_results = download_results(client, output_file_id)\n",
    "    \n",
    "    print(\"\\nâœ“ Batch evaluation complete!\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping live API call (RUN_BATCH_API = False)\")\n",
    "    print(\"   We'll use mock results in the next cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Use AutoBatcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-04 09:48:37.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mInitialized with batch_size=20, window=60.0s\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:37.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_enqueue_request\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mStarting 60.0s batch window timer\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:37.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_enqueue_request\u001b[0m:\u001b[36m208\u001b[0m - \u001b[34m\u001b[1mBatch size 20 reached\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting batch evaluation with autobatcher...\n",
      "ðŸ“¤ Sending 20 papers to autobatcher (batch_size=20)...\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25% | Preparing batch (20 requests)...\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50% | Uploading file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-04 09:48:38.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_submit_batch\u001b[0m:\u001b[36m277\u001b[0m - \u001b[34m\u001b[1mUploaded batch file: 359fa9cf-4c7a-4a17-ae74-fda7c8f55032\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  75% | Creating batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-04 09:48:38.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_submit_batch\u001b[0m:\u001b[36m287\u001b[0m - \u001b[34m\u001b[1mSubmitted batch 752f656c-e03f-40ca-b5a1-85f337d6ed4b with 20 requests\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:38.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m318\u001b[0m - \u001b[34m\u001b[1mPoller started with 1 active batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% | Submitted âœ“ (batch 752f656c-e03...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-04 09:48:44.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mBatch 752f656c-e03 status: completed (completed=20/20)\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:45.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_fetch_partial_results\u001b[0m:\u001b[36m423\u001b[0m - \u001b[34m\u001b[1mResolved 20 partial results, 0 pending\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:45.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m343\u001b[0m - \u001b[34m\u001b[1mBatch 752f656c-e03f-40ca-b5a1-85f337d6ed4b completed\u001b[0m\n",
      "\u001b[32m2026-02-04 09:48:45.687\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m363\u001b[0m - \u001b[34m\u001b[1mPoller finished\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Received 20 evaluations\n"
     ]
    }
   ],
   "source": [
    "# Using autobatcher for efficient batch processing\n",
    "# Install: pip install autobatcher\n",
    "\n",
    "import asyncio\n",
    "from autobatcher import BatchOpenAI\n",
    "\n",
    "async def evaluate_papers_with_autobatcher(papers, model, team_profile):\n",
    "    \"\"\"\n",
    "    Evaluate papers using autobatcher - waits for 20 requests before sending.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create autobatcher client - batch_size=20 means it waits for 20 requests\n",
    "    async with BatchOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "        batch_size=20,              # Wait until we have 20 requests\n",
    "        batch_window_seconds=60.0,  # Or send after 60 seconds if we don't hit 20\n",
    "        poll_interval_seconds=5.0,  # Check for results every 5 seconds\n",
    "        completion_window=\"1h\"\n",
    "    ) as client:\n",
    "        \n",
    "        # Create all evaluation tasks concurrently\n",
    "        async def evaluate_single_paper(paper):\n",
    "            prompt = create_evaluation_prompt(paper, team_profile)\n",
    "            response = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            # Parse the evaluation from the response\n",
    "            try:\n",
    "                evaluation = json.loads(content)\n",
    "                evaluation['paper_id'] = paper['id']\n",
    "                return evaluation\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âš ï¸ Failed to parse evaluation for paper {paper['id']}\")\n",
    "                return None\n",
    "        \n",
    "        # Send all 20 papers at once - autobatcher will batch them\n",
    "        print(f\"ðŸ“¤ Sending {len(papers)} papers to autobatcher (batch_size=20)...\")\n",
    "        tasks = [evaluate_single_paper(paper) for paper in papers]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Filter out any None results\n",
    "        evaluations = [r for r in results if r is not None]\n",
    "        print(f\"âœ“ Received {len(evaluations)} evaluations\")\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "# Run the evaluation\n",
    "print(\"ðŸš€ Starting batch evaluation with autobatcher...\")\n",
    "evaluations = await evaluate_papers_with_autobatcher(papers, MODEL_NAME, TEAM_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Results (For Workshop Demo)\n",
    "\n",
    "If you didn't run the actual API call, here are realistic mock results\n",
    "so you can see how the rest of the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock results that simulate what the API would return\n",
    "\n",
    "MOCK_EVALUATIONS = [\n",
    "    {\"paper_id\": \"2410.21276v1\", \"relevance_score\": 10,\n",
    "     \"key_insight\": \"Comprehensive survey of tool learning for LLM-based agents.\",\n",
    "     \"why_relevant\": \"Directly addresses AI agents and tool use.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2310.04406v2\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"AgentTuning improves agent abilities while maintaining general capabilities.\",\n",
    "     \"why_relevant\": \"Practical approach to building better AI agents.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2312.10997v3\", \"relevance_score\": 7,\n",
    "     \"key_insight\": \"Framework for governing agentic AI systems.\",\n",
    "     \"why_relevant\": \"Relevant to AI agents but unclear how technical.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2401.04088v2\", \"relevance_score\": 8,\n",
    "     \"key_insight\": \"Mixtral 8x7B achieves 6x faster inference with MoE.\",\n",
    "     \"why_relevant\": \"Directly relevant to inference optimization.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2403.03507v2\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"Claude 3 shows strong multimodal performance.\",\n",
    "     \"why_relevant\": \"Important benchmark for frontier models.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2305.10601v2\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"Tree of Thoughts enables deliberate problem solving.\",\n",
    "     \"why_relevant\": \"Core technique for AI agents.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2210.03629v3\", \"relevance_score\": 10,\n",
    "     \"key_insight\": \"ReAct synergizes reasoning and acting.\",\n",
    "     \"why_relevant\": \"Foundational work on AI agents.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2305.14314v2\", \"relevance_score\": 8,\n",
    "     \"key_insight\": \"QLoRA enables finetuning 65B models on single GPU.\",\n",
    "     \"why_relevant\": \"Relevant for efficient model adaptation.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2307.09288v2\", \"relevance_score\": 7,\n",
    "     \"key_insight\": \"Llama 2 provides open foundation models.\",\n",
    "     \"why_relevant\": \"Important but abstract focuses on training.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2309.16609v2\", \"relevance_score\": 8,\n",
    "     \"key_insight\": \"Mistral 7B outperforms larger models.\",\n",
    "     \"why_relevant\": \"Relevant to inference efficiency.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2305.18290v2\", \"relevance_score\": 6,\n",
    "     \"key_insight\": \"DPO optimizes for preferences without reward modeling.\",\n",
    "     \"why_relevant\": \"Training technique - less relevant.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2402.03300v2\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"Self-Discover lets LLMs compose reasoning structures.\",\n",
    "     \"why_relevant\": \"Novel prompting for agents.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2310.06825v2\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"Self-RAG trains models to retrieve and self-critique.\",\n",
    "     \"why_relevant\": \"Practical for reducing hallucinations.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2401.02954v2\", \"relevance_score\": 7,\n",
    "     \"key_insight\": \"DeepSeek LLM trained on 2T tokens.\",\n",
    "     \"why_relevant\": \"Abstract focuses on training.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2308.12950v3\", \"relevance_score\": 8,\n",
    "     \"key_insight\": \"Code Llama provides SOTA code generation.\",\n",
    "     \"why_relevant\": \"Practical for AI-assisted coding.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2312.11805v3\", \"relevance_score\": 8,\n",
    "     \"key_insight\": \"Gemini models trained across modalities.\",\n",
    "     \"why_relevant\": \"Benchmark for multimodal development.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2310.12931v2\", \"relevance_score\": 5,\n",
    "     \"key_insight\": \"Llemma extends Code Llama for mathematics.\",\n",
    "     \"why_relevant\": \"Specialized - less relevant.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2309.05463v2\", \"relevance_score\": 7,\n",
    "     \"key_insight\": \"phi-1.5 shows data quality trains capable small models.\",\n",
    "     \"why_relevant\": \"Training insights - unclear for deployment.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2401.10020v1\", \"relevance_score\": 9,\n",
    "     \"key_insight\": \"Survey of resource-efficient LLM techniques.\",\n",
    "     \"why_relevant\": \"Directly addresses inference optimization.\"},\n",
    "    \n",
    "    {\"paper_id\": \"2403.08295v2\", \"relevance_score\": 7,\n",
    "     \"key_insight\": \"SIMA agents follow instructions in games.\",\n",
    "     \"why_relevant\": \"Interesting but gaming-focused.\"}\n",
    "]\n",
    "\n",
    "# Use mock results if we didn't run the API\n",
    "if not RUN_BATCH_API or not API_KEY:\n",
    "    evaluations = MOCK_EVALUATIONS\n",
    "    print(f\"âœ“ Using {len(evaluations)} mock evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Results with Paper Data\n",
    "\n",
    "Now let's combine the evaluations with the original paper data\n",
    "so we have everything in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged 20 papers with evaluations\n"
     ]
    }
   ],
   "source": [
    "def merge_results_with_papers(papers: list[dict], evaluations: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Combine paper data with evaluation results.\n",
    "    Returns merged list sorted by relevance score (highest first).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create lookup for evaluations by paper_id\n",
    "    eval_lookup = {e['paper_id']: e for e in evaluations}\n",
    "    \n",
    "    merged = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        if paper_id in eval_lookup:\n",
    "            # Combine paper data with evaluation\n",
    "            combined = {**paper, **eval_lookup[paper_id]}\n",
    "            combined.pop('paper_id', None)  # Remove redundant field\n",
    "            merged.append(combined)\n",
    "    \n",
    "    # Sort by relevance score (highest first)\n",
    "    merged.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Merge everything together\n",
    "results = merge_results_with_papers(papers, evaluations)\n",
    "print(f\"âœ“ Merged {len(results)} papers with evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top 10 Papers\n",
    "\n",
    "Finally! Let's see which papers the AI thinks our team should read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 10 PAPERS FOR YOUR TEAM (score >= 7)\n",
      "================================================================================\n",
      "\n",
      "â­ [8/10] Accelerating Scientific Research with Gemini: Case Studies and Co...\n",
      "   ðŸ’¡ The paper demonstrates practical, real-world applications of advanced LLMs in scientific research through collaborative problem-solving and novel AI-assisted methodologies.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03837v1\n",
      "\n",
      "â­ [8/10] Conformal Thinking: Risk Control for Reasoning on a Compute Budge...\n",
      "   ðŸ’¡ The paper introduces a risk-control framework for adaptive reasoning in LLMs that optimizes compute usage by intelligently stopping reasoning when confidence is high or problems are unsolvable, improving efficiency while maintaining accuracy.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03814v1\n",
      "\n",
      "â­ [8/10] Antidistillation Fingerprinting\n",
      "   ðŸ’¡ Antidistillation fingerprinting enables strong detection of model distillation with minimal impact on student model utility by aligning fingerprinting with the student's learning dynamics.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03812v1\n",
      "\n",
      "â­ [8/10] Context Compression via Explicit Information Transmission\n",
      "   ðŸ’¡ ComprExIT enables more effective long-context compression in LLMs by decoupling compression from self-attention through explicit, coordinated information transmission across layers and tokens.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03784v1\n",
      "\n",
      "â­ [8/10] QVLA: Not All Channels Are Equal in Vision-Language-Action Model'...\n",
      "   ðŸ’¡ QVLA introduces a channel-wise, action-sensitive quantization framework that significantly reduces VRAM usage and improves inference speed for Vision-Language-Action models without sacrificing performance, enabling deployment on resource-constrained robotic platforms.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03782v1\n",
      "\n",
      "â­ [8/10] Reasoning Cache: Continual Improvement Over Long Horizons via Sho...\n",
      "   ðŸ’¡ Reasoning Cache enables LLMs to continually improve over long horizons through iterative decoding, enhancing test-time performance without additional training.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03773v1\n",
      "\n",
      "â­ [8/10] Training Multi-Turn Search Agent via Contrastive Dynamic Branch S...\n",
      "   ðŸ’¡ BranPO improves multi-turn agent performance by using contrastive supervision and adaptive branching to reduce credit assignment ambiguity in long-horizon tasks.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03719v1\n",
      "\n",
      "â­ [8/10] Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient ...\n",
      "   ðŸ’¡ SemanticSpec improves inference efficiency by using semantic-level verification instead of token-level checks, enabling faster and more effective speculative decoding.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03708v1\n",
      "\n",
      "â­ [7/10] Understanding and Exploiting Weight Update Sparsity for Communica...\n",
      "   ðŸ’¡ PULSE enables highly efficient, lossless weight synchronization in distributed RL by exploiting sparsity in model updates, reducing communication bandwidth by over 100x without compromising performance.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03839v1\n",
      "\n",
      "â­ [7/10] Fast-Slow Efficient Training for Multimodal Large Language Models...\n",
      "   ðŸ’¡ DualSpeed enables efficient training of multimodal LLMs by combining fast visual token pruning with slow-mode self-distillation to maintain performance while accelerating training.\n",
      "   ðŸ“Ž http://arxiv.org/abs/2602.03815v1\n",
      "\n",
      "================================================================================\n",
      "Showing 11 papers with relevance score >= 7\n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 most relevant papers\n",
    "TOP_N = 10\n",
    "MIN_SCORE = 7  # Only show papers with score >= 7\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {TOP_N} PAPERS FOR YOUR TEAM (score >= {MIN_SCORE})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shown = 0\n",
    "for paper in results:\n",
    "    score = paper.get('relevance_score', 0)\n",
    "    \n",
    "    if score < MIN_SCORE:\n",
    "        continue\n",
    "    \n",
    "    shown += 1\n",
    "    if shown > TOP_N:\n",
    "        break\n",
    "    \n",
    "    # Score indicator\n",
    "    if score >= 9:\n",
    "        indicator = \"ðŸ”¥\"\n",
    "    elif score >= 7:\n",
    "        indicator = \"â­\"\n",
    "    else:\n",
    "        indicator = \"ðŸ“„\"\n",
    "    \n",
    "    print(f\"\\n{indicator} [{score}/10] {paper['title'][:65]}{'...' if len(paper['title']) > 65 else ''}\")\n",
    "    print(f\"   ðŸ’¡ {paper.get('key_insight', 'N/A')}\")\n",
    "    print(f\"   ðŸ“Ž {paper['url']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Showing {shown} papers with relevance score >= {MIN_SCORE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Understanding the Results\n",
    "\n",
    "Let's analyze what we got and discuss the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The JSON Structure\n",
    "\n",
    "Each evaluated paper now contains both the original arXiv data\n",
    "and the AI-generated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Complete structure of a top-rated paper:\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"2602.03837v1\",\n",
      "  \"title\": \"Accelerating Scientific Research with Gemini: Case...\",\n",
      "  \"authors\": [\n",
      "    \"David P. Woodruff\",\n",
      "    \"Vincent Cohen-Addad\"\n",
      "  ],\n",
      "  \"abstract\": \"Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific ...\",\n",
      "  \"published\": \"2026-02-03T18:56:17+00:00\",\n",
      "  \"url\": \"http://arxiv.org/abs/2602.03837v1\",\n",
      "  \"relevance_score\": 8,\n",
      "  \"key_insight\": \"The paper demonstrates practical, real-world applications of advanced LLMs in scientific research through collaborative problem-solving and novel AI-assisted methodologies.\",\n",
      "  \"why_relevant\": \"It highlights effective human-AI collaboration techniques, inference optimization through iterative refinement, and practical use of LLMs in complex domains, aligning with the team's interests in AI agents, prompt engineering, and inference optimization.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Look at the complete structure of a top paper\n",
    "print(\"ðŸ“Š Complete structure of a top-rated paper:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_paper = results[0]\n",
    "print(json.dumps({\n",
    "    'id': top_paper['id'],\n",
    "    'title': top_paper['title'][:50] + '...',\n",
    "    'authors': top_paper['authors'][:2],\n",
    "    'abstract': top_paper['abstract'][:100] + '...',\n",
    "    'published': top_paper['published'],\n",
    "    'url': top_paper['url'],\n",
    "    'relevance_score': top_paper['relevance_score'],\n",
    "    'key_insight': top_paper['key_insight'],\n",
    "    'why_relevant': top_paper['why_relevant']\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Distribution\n",
    "\n",
    "Let's see how the scores are distributed across all papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Score Distribution:\n",
      "========================================\n",
      "High (8-10)     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (8)\n",
      "Borderline (7)  | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (7)\n",
      "Low (0-6)       | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (5)\n",
      "\n",
      "Average score: 7.0\n"
     ]
    }
   ],
   "source": [
    "# Count papers in each score range\n",
    "score_ranges = {\n",
    "    'High (8-10)': 0,\n",
    "    'Borderline (7)': 0,\n",
    "    'Low (0-6)': 0\n",
    "}\n",
    "\n",
    "for paper in results:\n",
    "    score = paper.get('relevance_score', 0)\n",
    "    if score >= 8:\n",
    "        score_ranges['High (8-10)'] += 1\n",
    "    elif score >= 7:\n",
    "        score_ranges['Borderline (7)'] += 1\n",
    "    else:\n",
    "        score_ranges['Low (0-6)'] += 1\n",
    "\n",
    "print(\"ðŸ“ˆ Score Distribution:\")\n",
    "print(\"=\"*40)\n",
    "for range_name, count in score_ranges.items():\n",
    "    bar = \"â–ˆ\" * count\n",
    "    print(f\"{range_name:15} | {bar} ({count})\")\n",
    "\n",
    "# Average score\n",
    "avg_score = sum(p.get('relevance_score', 0) for p in results) / len(results)\n",
    "print(f\"\\nAverage score: {avg_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Score Distribution Problem\n",
    "\n",
    "Notice what the average score and most papers cluster between 7-10. This is a common problem with LLM evaluation!\n",
    "\n",
    "The model is being \"polite\" and hedging - everything seems \"pretty good.\" With such low variance, it's hard to prioritize which papers to actually read.\n",
    "\n",
    "**We need external signals to differentiate!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Re-evaluating Borderline Papers\n",
    "\n",
    "Papers with score **7** are borderline. We'll:\n",
    "1. Download the full PDF\n",
    "2. Send it to the LLM for re-evaluation\n",
    "3. Get a revised score based on the full content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 borderline papers (score=7) to re-evaluate:\n",
      "  â€¢ Understanding and Exploiting Weight Update Sparsity for...\n",
      "  â€¢ Fast-Slow Efficient Training for Multimodal Large Langu...\n",
      "  â€¢ Bridging Online and Offline RL: Contextual Bandit Learn...\n",
      "  â€¢ UniGeM: Unifying Data Mixing and Selection via Geometri...\n",
      "  â€¢ SWE-Refactor: A Repository-Level Benchmark for Real-Wor...\n",
      "  â€¢ Cognitively Diverse Multiple-Choice Question Generation...\n",
      "  â€¢ Conflict-Resolving and Sharpness-Aware Minimization for...\n"
     ]
    }
   ],
   "source": [
    "# Find borderline papers (score = 7)\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from tools import get_borderline_papers\n",
    "\n",
    "borderline = get_borderline_papers(results, score=7)\n",
    "\n",
    "print(f\"Found {len(borderline)} borderline papers (score=7) to re-evaluate:\")\n",
    "for p in borderline:\n",
    "    print(f\"  â€¢ {p['title'][:55]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading PDFs for borderline papers...\n",
      "  [1/7] Understanding and Exploiting Weight Update Sp...\n",
      "  âœ“ Extracted 32,634 characters from 2602.03839v1\n",
      "  [2/7] Fast-Slow Efficient Training for Multimodal L...\n",
      "  âœ“ Extracted 44,048 characters from 2602.03815v1\n",
      "  [3/7] Bridging Online and Offline RL: Contextual Ba...\n",
      "  âœ“ Extracted 48,676 characters from 2602.03806v1\n",
      "  [4/7] UniGeM: Unifying Data Mixing and Selection vi...\n",
      "  âœ“ Extracted 37,680 characters from 2602.03772v1\n",
      "  [5/7] SWE-Refactor: A Repository-Level Benchmark fo...\n",
      "  âœ“ Extracted 50,336 characters from 2602.03712v1\n",
      "  [6/7] Cognitively Diverse Multiple-Choice Question ...\n",
      "  âœ“ Extracted 30,333 characters from 2602.03704v1\n",
      "  [7/7] Conflict-Resolving and Sharpness-Aware Minimi...\n",
      "  âœ“ Extracted 46,921 characters from 2602.03696v1\n",
      "\n",
      "âœ“ Extracted text from 7/7 papers\n"
     ]
    }
   ],
   "source": [
    "from tools import get_paper_for_deep_analysis\n",
    "\n",
    "# Step 1: Download PDFs for all borderline papers\n",
    "print(\"ðŸ“¥ Downloading PDFs for borderline papers...\")\n",
    "papers_with_text = []\n",
    "\n",
    "for i, paper in enumerate(borderline):\n",
    "    print(f\"  [{i+1}/{len(borderline)}] {paper['title'][:45]}...\")\n",
    "    result = get_paper_for_deep_analysis(paper, output_dir=\"../papers\")\n",
    "    papers_with_text.append(result)\n",
    "\n",
    "success = sum(1 for p in papers_with_text if p.get('full_text'))\n",
    "print(f\"\\nâœ“ Extracted text from {success}/{len(borderline)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Submitting batch re-evaluation...\n",
      "Submitting batch of 7 papers for re-evaluation...\n",
      "Batch ID: 823042a2-0b8b-4467-8d04-f4b1cbee7403\n",
      "Waiting for completion...\n",
      "Status: in_progress | Progress: 0/7\n",
      "Status: completed | Progress: 7/7\n",
      "\n",
      "âœ“ Got 7 re-evaluation results\n"
     ]
    }
   ],
   "source": [
    "from evaluate_papers import reevaluate_papers_batch\n",
    "\n",
    "# Step 2: Batch re-evaluate all papers with the LLM\n",
    "print(\"ðŸ¤– Submitting batch re-evaluation...\")\n",
    "reeval_results = reevaluate_papers_batch(papers_with_text, verbose=True)\n",
    "\n",
    "print(f\"\\nâœ“ Got {len(reeval_results)} re-evaluation results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RE-EVALUATION RESULTS\n",
      "===========================================================================\n",
      "Paper                                    |  Before |   After |  Change\n",
      "---------------------------------------------------------------------------\n",
      "Understanding and Exploiting Weight U... |       7 |       8 |      +1\n",
      "Fast-Slow Efficient Training for Mult... |       7 |       6 |      -1\n",
      "Bridging Online and Offline RL: Conte... |       7 |       8 |      +1\n",
      "UniGeM: Unifying Data Mixing and Sele... |       7 |       8 |      +1\n",
      "SWE-Refactor: A Repository-Level Benc... |       7 |       7 |       0\n",
      "Cognitively Diverse Multiple-Choice Q... |       7 |       8 |      +1\n",
      "Conflict-Resolving and Sharpness-Awar... |       7 |       8 |      +1\n"
     ]
    }
   ],
   "source": [
    "# Merge re-evaluation results with original papers\n",
    "reeval_lookup = {r['paper_id']: r for r in reeval_results}\n",
    "\n",
    "print(\"ðŸ“Š RE-EVALUATION RESULTS\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Paper':<40} | {'Before':>7} | {'After':>7} | {'Change':>7}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "final_results = []\n",
    "for paper in papers_with_text:\n",
    "    paper_id = paper['id']\n",
    "    original_score = paper['relevance_score']\n",
    "    \n",
    "    if paper_id in reeval_lookup:\n",
    "        revised = reeval_lookup[paper_id]\n",
    "        new_score = revised.get('revised_score', original_score)\n",
    "        change = new_score - original_score\n",
    "        change_str = f\"+{change}\" if change > 0 else str(change)\n",
    "        \n",
    "        final_results.append({\n",
    "            **paper,\n",
    "            'revised_score': new_score,\n",
    "            'key_insight': revised.get('key_insight'),\n",
    "            'why_revised': revised.get('why_revised')\n",
    "        })\n",
    "    else:\n",
    "        new_score = \"N/A\"\n",
    "        change_str = \"â€”\"\n",
    "        final_results.append({**paper, 'revised_score': None})\n",
    "    \n",
    "    title = paper['title'][:37] + \"...\" if len(paper['title']) > 37 else paper['title']\n",
    "    print(f\"{title:<40} | {original_score:>7} | {str(new_score):>7} | {change_str:>7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– FINAL RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "âœ… RECOMMEND (13 papers):\n",
      "   From initial evaluation (score 8-10):\n",
      "     â€¢ Accelerating Scientific Research with Gemini: Case...\n",
      "     â€¢ Conformal Thinking: Risk Control for Reasoning on ...\n",
      "     â€¢ Antidistillation Fingerprinting...\n",
      "   Upgraded after reading full paper:\n",
      "     â€¢ Understanding and Exploiting Weight Update Sp... (7â†’8)\n",
      "     â€¢ Bridging Online and Offline RL: Contextual Ba... (7â†’8)\n",
      "     â€¢ UniGeM: Unifying Data Mixing and Selection vi... (7â†’8)\n",
      "     â€¢ Cognitively Diverse Multiple-Choice Question ... (7â†’8)\n",
      "     â€¢ Conflict-Resolving and Sharpness-Aware Minimi... (7â†’8)\n",
      "\n",
      "â­ï¸  SKIP (2 papers stayed borderline/downgraded):\n",
      "     â€¢ Fast-Slow Efficient Training for Multimodal L... (7â†’6)\n",
      "     â€¢ SWE-Refactor: A Repository-Level Benchmark fo... (7â†’7)\n"
     ]
    }
   ],
   "source": [
    "# Final recommendations\n",
    "print(\"\\nðŸ¤– FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Original high-scorers\n",
    "high_original = [p for p in results if p['relevance_score'] >= 8]\n",
    "\n",
    "# Borderline papers that got upgraded\n",
    "upgraded = [p for p in final_results if p.get('revised_score') and p['revised_score'] >= 8]\n",
    "\n",
    "# Borderline papers that stayed or went down\n",
    "not_upgraded = [p for p in final_results if p.get('revised_score') and p['revised_score'] < 8]\n",
    "\n",
    "print(f\"\\nâœ… RECOMMEND ({len(high_original) + len(upgraded)} papers):\")\n",
    "print(\"   From initial evaluation (score 8-10):\")\n",
    "for p in high_original[:3]:\n",
    "    print(f\"     â€¢ {p['title'][:50]}...\")\n",
    "\n",
    "if upgraded:\n",
    "    print(\"   Upgraded after reading full paper:\")\n",
    "    for p in upgraded:\n",
    "        print(f\"     â€¢ {p['title'][:45]}... ({p['relevance_score']}â†’{p['revised_score']})\")\n",
    "\n",
    "if not_upgraded:\n",
    "    print(f\"\\nâ­ï¸  SKIP ({len(not_upgraded)} papers stayed borderline/downgraded):\")\n",
    "    for p in not_upgraded[:3]:\n",
    "        print(f\"     â€¢ {p['title'][:45]}... ({p['relevance_score']}â†’{p.get('revised_score')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gathering User Feedback\n",
    "\n",
    "The system made recommendations, but were they actually useful? User feedback helps:\n",
    "1. **Validate** the current evaluation quality\n",
    "2. **Improve** future evaluations by learning what the team actually finds valuable\n",
    "3. **Track** patterns in what the LLM gets wrong\n",
    "\n",
    "This creates a feedback loop:\n",
    "```\n",
    "LLM Evaluation â†’ Recommendations â†’ User Feedback â†’ Better Prompts/Fine-tuning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h3>ðŸ“ Rate These Recommendations</h3>\n",
       "    <p style=\"color: #666;\">Click a rating for each paper. Your feedback helps improve future recommendations.</p>\n",
       "    <hr>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd46c90525464fea8af7cb25a8863dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eb237b34974aa7ab7ca028bbaaf5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ðŸ‘Ž Not Useful', layout=Layout(width='120px'), style=ButtonStyle()), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fecb0cdea10474a9aec50dff1ea9be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef3e83634ca4d7c99588cac72e3d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ðŸ‘Ž Not Useful', layout=Layout(width='120px'), style=ButtonStyle()), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5aa35bfd48743e99375ec3f7a883de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10826efe54de4b688cb345785e0b0362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ðŸ‘Ž Not Useful', layout=Layout(width='120px'), style=ButtonStyle()), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa20d631bb29426f8b755b65a895477d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff7af4ac7a040929b6ef80f3f094f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ðŸ‘Ž Not Useful', layout=Layout(width='120px'), style=ButtonStyle()), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e7a3876bf54ec8b9238b8d51028abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefb65d455964236846437f20051cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ðŸ‘Ž Not Useful', layout=Layout(width='120px'), style=ButtonStyle()), Button(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18a4dd669d24ed69dc206acd8bfe565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='ðŸ“Š Show Summary', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb3d9e5d8374f16b070b4109529680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive feedback collection using Jupyter widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Store feedback\n",
    "collected_feedback = []\n",
    "\n",
    "def create_feedback_ui(papers: list[dict]):\n",
    "    \"\"\"Create an interactive feedback UI for rating papers.\"\"\"\n",
    "\n",
    "    # Clear previous output and feedback to avoid duplicates on re-run\n",
    "    clear_output(wait=True)\n",
    "    collected_feedback.clear()\n",
    "\n",
    "    # Header\n",
    "    display(HTML(\"\"\"\n",
    "    <h3>ðŸ“ Rate These Recommendations</h3>\n",
    "    <p style=\"color: #666;\">Click a rating for each paper. Your feedback helps improve future recommendations.</p>\n",
    "    <hr>\n",
    "    \"\"\"))\n",
    "    \n",
    "    for i, paper in enumerate(papers[:5]):\n",
    "        paper_id = paper['id']\n",
    "        title = paper['title'][:60] + \"...\" if len(paper['title']) > 60 else paper['title']\n",
    "        score = paper.get('relevance_score') or paper.get('revised_score', '?')\n",
    "        insight = paper.get('key_insight', 'No insight available')[:80]\n",
    "        \n",
    "        # Paper info display\n",
    "        paper_html = widgets.HTML(f\"\"\"\n",
    "        <div style=\"background: #f5f5f5; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "            <b style=\"font-size: 14px;\">{i+1}. {title}</b><br>\n",
    "            <span style=\"color: #666; font-size: 12px;\">LLM Score: {score} | {insight}...</span>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Rating buttons\n",
    "        btn_not_useful = widgets.Button(description='ðŸ‘Ž Not Useful', layout=widgets.Layout(width='120px'))\n",
    "        btn_somewhat = widgets.Button(description='ðŸ¤” Okay', layout=widgets.Layout(width='100px'))\n",
    "        btn_useful = widgets.Button(description='ðŸ‘ Useful', layout=widgets.Layout(width='100px'))\n",
    "        btn_very_useful = widgets.Button(description='â­ Must Read', layout=widgets.Layout(width='120px'))\n",
    "        \n",
    "        # Status label\n",
    "        status = widgets.HTML(value=\"<i style='color:#999'>Not rated</i>\")\n",
    "        \n",
    "        # Button click handlers\n",
    "        def make_handler(pid, ptitle, rating, rating_text, status_widget, buttons):\n",
    "            def handler(b):\n",
    "                collected_feedback.append({\n",
    "                    'paper_id': pid,\n",
    "                    'title': ptitle,\n",
    "                    'user_rating': rating,\n",
    "                    'rating_text': rating_text\n",
    "                })\n",
    "                status_widget.value = f\"<span style='color:green'>âœ“ {rating_text}</span>\"\n",
    "                for btn in buttons:\n",
    "                    btn.disabled = True\n",
    "            return handler\n",
    "        \n",
    "        buttons = [btn_not_useful, btn_somewhat, btn_useful, btn_very_useful]\n",
    "        btn_not_useful.on_click(make_handler(paper_id, title, 1, 'Not Useful', status, buttons))\n",
    "        btn_somewhat.on_click(make_handler(paper_id, title, 2, 'Okay', status, buttons))\n",
    "        btn_useful.on_click(make_handler(paper_id, title, 3, 'Useful', status, buttons))\n",
    "        btn_very_useful.on_click(make_handler(paper_id, title, 4, 'Must Read', status, buttons))\n",
    "        \n",
    "        button_box = widgets.HBox([btn_not_useful, btn_somewhat, btn_useful, btn_very_useful, status])\n",
    "        display(paper_html)\n",
    "        display(button_box)\n",
    "    \n",
    "    display(HTML(\"<hr>\"))\n",
    "    submit_btn = widgets.Button(description='ðŸ“Š Show Summary', button_style='primary')\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def show_summary(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            if collected_feedback:\n",
    "                print(f\"Collected {len(collected_feedback)} ratings:\")\n",
    "                for f in collected_feedback:\n",
    "                    print(f\"  {f['title'][:40]}... â†’ {f['rating_text']}\")\n",
    "            else:\n",
    "                print(\"No ratings yet!\")\n",
    "    \n",
    "    submit_btn.on_click(show_summary)\n",
    "    display(submit_btn)\n",
    "    display(output)\n",
    "\n",
    "# Show UI for recommended papers\n",
    "recommended = [p for p in results if p['relevance_score'] >= 8]\n",
    "create_feedback_ui(recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š FEEDBACK ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Rating Distribution:\n",
      "  ðŸ‘Ž Not Useful: â–ˆ (1)\n",
      "  ðŸ¤” Okay:  (0)\n",
      "  ðŸ‘ Useful: â–ˆâ–ˆ (2)\n",
      "  â­ Must Read: â–ˆâ–ˆ (2)\n",
      "\n",
      "Accuracy: 4/5 (80%) were useful\n"
     ]
    }
   ],
   "source": [
    "# Analyze collected feedback\n",
    "print(\"ðŸ“Š FEEDBACK ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if collected_feedback:\n",
    "    from collections import Counter\n",
    "    ratings = Counter(f['user_rating'] for f in collected_feedback)\n",
    "    \n",
    "    print(\"\\nRating Distribution:\")\n",
    "    labels = {1: 'ðŸ‘Ž Not Useful', 2: 'ðŸ¤” Okay', 3: 'ðŸ‘ Useful', 4: 'â­ Must Read'}\n",
    "    for rating, label in labels.items():\n",
    "        count = ratings.get(rating, 0)\n",
    "        bar = 'â–ˆ' * count\n",
    "        print(f\"  {label}: {bar} ({count})\")\n",
    "    \n",
    "    useful = sum(1 for f in collected_feedback if f['user_rating'] >= 3)\n",
    "    total = len(collected_feedback)\n",
    "    print(f\"\\nAccuracy: {useful}/{total} ({100*useful/total:.0f}%) were useful\")\n",
    "else:\n",
    "    print(\"No feedback collected yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Closing the Loop â€” Self-Improving Agents\n",
    "\n",
    "This is where it gets interesting. We now have **real user feedback** on our recommendations. Instead of just hoping the LLM gets it right, we can:\n",
    "\n",
    "1. **Detect Patterns** â€” Find systematic errors (e.g., \"LLM keeps recommending training papers we don't care about\")\n",
    "2. **Update the Prompt** â€” Automatically suggest additions to `TEAM_PROFILE` based on what users actually value\n",
    "3. **Measure Improvement** â€” Track accuracy over time as the prompt evolves\n",
    "\n",
    "This is the core loop for building agents that get better:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                         â”‚\n",
    "â”‚   LLM Evaluates  â†’  User Rates  â†’  Find Patterns  â†’    â”‚\n",
    "â”‚        â†‘                                      â”‚         â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Update Prompt  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The agent isn't just running inference â€” it's **learning from feedback** to improve its own instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
