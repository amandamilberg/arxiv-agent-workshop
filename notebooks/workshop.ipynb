{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline - Understanding Batch Inference for Paper Evaluation\n",
    "\n",
    "In this workshop, we'll build a simple paper evaluation system that:\n",
    "1. **Fetches** recent AI research papers from arXiv\n",
    "2. **Evaluates** each paper's relevance using the Batch API\n",
    "3. **Ranks** papers to find the most interesting ones for our team\n",
    "\n",
    "This is our **baseline** - a fixed pipeline with no decision-making.\n",
    "In later parts, we'll transform this into a true AI agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import our libraries and configure the environment.\n",
    "\n",
    "**What we're using:**\n",
    "- `arxiv` - Python client for searching arXiv papers\n",
    "- `openai` - Client for the OpenAI-compatible Batch API. We will be using the Doubleword Batch API in our example.\n",
    "- `dotenv` - Load API keys from `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import arxiv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add src directory to path so we can import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our Doubleword API Key\n",
    "\n",
    "Now we need to create a Doubleword API Key. \n",
    "\n",
    "1. Sign up at https://app.doubleword.com\n",
    "2. Create an API key, save this in a safe place\n",
    "3. Copy the `.env.example` file in the same directory, name it `.env` and add your api key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API key loaded (ends with ...1Ok4)\n",
      "âœ“ Model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "âœ“ Base URL: https://api.doubleword.ai/v1\n"
     ]
    }
   ],
   "source": [
    "# Configuration - these come from your .env file\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\")  # Optional: for non-OpenAI providers\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Verify we have an API key\n",
    "if not API_KEY:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found in environment!\")\n",
    "    print(\"   Create a .env file with your API key, or we'll use sample data.\")\n",
    "else:\n",
    "    print(f\"âœ“ API key loaded (ends with ...{API_KEY[-4:]})\")\n",
    "    print(f\"âœ“ Model: {MODEL_NAME}\")\n",
    "    if BASE_URL:\n",
    "        print(f\"âœ“ Base URL: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a quick test to make sure that we have established connectivity to Doubleword. The Doubleword follows the openai spec, so let's see which models we have availabile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  â€¢ Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\n",
      "  â€¢ Qwen/Qwen3-Embedding-8B\n",
      "  â€¢ Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model in client.models.list():\n",
    "    print(f\"  â€¢ {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We're Building\n",
    "\n",
    "Our goal is to help an AI engineering team stay current with research.\n",
    "Every day, hundreds of papers are published - we can't read them all!\n",
    "\n",
    "**The Pipeline:**\n",
    "```\n",
    "arXiv API â†’ Fetch Papers â†’ Batch API â†’ Evaluate Relevance â†’ Ranked List\n",
    "```\n",
    "\n",
    "**Why Batch API?**\n",
    "- 50% cheaper than real-time API calls\n",
    "- Process many papers in parallel\n",
    "- Perfect for non-interactive workloads\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch Papers from arXiv\n",
    "\n",
    "arXiv is a free repository of research papers, widely used in AI/ML.\n",
    "We'll search for recent papers matching our keywords.\n",
    "\n",
    "Let's look at the code that fetches papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers(keywords: list[str], max_results: int = 20, days_back: int = 7) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch recent papers from arXiv matching the given keywords.\n",
    "    \n",
    "    Args:\n",
    "        keywords: Search terms (e.g., [\"LLM\", \"transformers\"])\n",
    "        max_results: Maximum papers to return\n",
    "        days_back: How far back to search\n",
    "    \n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate cutoff date\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    \n",
    "    # Build search query: \"keyword1\" OR \"keyword2\" OR ...\n",
    "    # Quotes ensure exact phrase matching\n",
    "    query = \" OR \".join([f'\"{kw}\"' for kw in keywords])\n",
    "    print(f\"ðŸ” Searching arXiv for: {query}\")\n",
    "    print(f\"ðŸ“… Looking for papers since: {cutoff_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Configure the arXiv search\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,  # Most recent first\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Execute search and collect results\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for paper in client.results(search):\n",
    "        # Only include papers from our time window\n",
    "        paper_date = paper.published.replace(tzinfo=None)\n",
    "        if paper_date >= cutoff_date:\n",
    "            papers.append({\n",
    "                'id': paper.entry_id.split('/')[-1],  # e.g., \"2401.12345v1\"\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'abstract': paper.summary,\n",
    "                'published': paper.published.isoformat(),\n",
    "                'url': paper.entry_id\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ“ Found {len(papers)} papers\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Fetch Live Papers from arXiv\n",
    "\n",
    "This searches arXiv in real-time. Results vary based on what's been published recently. You can tweak `max_results` to get more or less papers to search from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Using sample data (set USE_LIVE_DATA = True to fetch live)\n"
     ]
    }
   ],
   "source": [
    "# Define what we're interested in\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"large language models\",\n",
    "    \"LLM agents\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "# Fetch papers (set to False to use sample data instead)\n",
    "USE_LIVE_DATA = False  # Change to True to fetch from arXiv\n",
    "\n",
    "if USE_LIVE_DATA:\n",
    "    papers = get_papers(SEARCH_KEYWORDS, max_results=20)\n",
    "else:\n",
    "    print(\"ðŸ“‚ Using sample data (set USE_LIVE_DATA = True to fetch live)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load Sample Papers (Recommended for Workshop)\n",
    "\n",
    "For consistent results during the workshop, we'll use pre-saved sample papers.\n",
    "This also works offline and doesn't require waiting for arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 20 sample papers\n"
     ]
    }
   ],
   "source": [
    "def load_sample_papers(filepath: str = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load pre-saved sample papers for offline demos.\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        # Default path relative to notebooks directory\n",
    "        filepath = Path.cwd().parent / \"sample_data\" / \"papers.json\"\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        papers = json.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(papers)} sample papers\")\n",
    "    return papers\n",
    "\n",
    "# Load sample papers if we didn't fetch live ones\n",
    "if not USE_LIVE_DATA:\n",
    "    papers = load_sample_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Papers\n",
    "\n",
    "Let's see what we got back from arXiv (or our sample data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 20\n",
      "\n",
      "ðŸ“„ Structure of a paper object:\n",
      "   Keys: ['id', 'title', 'authors', 'abstract', 'published', 'url']\n"
     ]
    }
   ],
   "source": [
    "# How many papers do we have?\n",
    "print(f\"Total papers: {len(papers)}\")\n",
    "print()\n",
    "\n",
    "# Look at the structure of one paper\n",
    "print(\"ðŸ“„ Structure of a paper object:\")\n",
    "print(f\"   Keys: {list(papers[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIRST 5 PAPERS\n",
      "================================================================================\n",
      "\n",
      "1. Efficient Inference for Large Language Models: A Survey\n",
      "   ID: 2401.00001v1\n",
      "   Authors: Alice Chen, Bob Williams, Carol Davis\n",
      "   Abstract: Large Language Models (LLMs) have revolutionized natural language processing but face significant challenges in deployment due to their computational ...\n",
      "\n",
      "2. ReAct Agents: Combining Reasoning and Acting in Language Models\n",
      "   ID: 2401.00002v1\n",
      "   Authors: David Park, Emily Zhang\n",
      "   Abstract: We present an improved framework for building AI agents that interleave reasoning traces with actions. Our approach enables language models to generat...\n",
      "\n",
      "3. Prompt Engineering Best Practices for Production Systems\n",
      "   ID: 2401.00003v1\n",
      "   Authors: Frank Miller, Grace Lee, Henry Wang\n",
      "   Abstract: As large language models become integral to production systems, effective prompt engineering has emerged as a critical skill. This paper presents a sy...\n",
      "\n",
      "4. Scaling Laws for Fine-tuning Language Models\n",
      "   ID: 2401.00004v1\n",
      "   Authors: Isabella Rodriguez, James Kim\n",
      "   Abstract: We investigate scaling laws that govern the fine-tuning of large language models. Through extensive experiments across model sizes from 125M to 70B pa...\n",
      "\n",
      "5. Tool Use in Large Language Models: A Comprehensive Survey\n",
      "   ID: 2401.00005v1\n",
      "   Authors: Kevin Brown, Laura Martinez, Michael Taylor\n",
      "   Abstract: The ability of large language models to use external tools represents a significant advance toward artificial general intelligence. This survey system...\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 papers\n",
    "print(\"=\"*80)\n",
    "print(\"FIRST 5 PAPERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, paper in enumerate(papers[:5], 1):\n",
    "    print(f\"\\n{i}. {paper['title'][:70]}...\" if len(paper['title']) > 70 else f\"\\n{i}. {paper['title']}\")\n",
    "    print(f\"   ID: {paper['id']}\")\n",
    "    print(f\"   Authors: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}\")\n",
    "    print(f\"   Abstract: {paper['abstract'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Got Back\n",
    "\n",
    "Each paper is a dictionary with:\n",
    "- **id**: arXiv identifier (useful for linking back to papers)\n",
    "- **title**: The paper's title\n",
    "- **authors**: List of author names\n",
    "- **abstract**: Summary of the paper (this is what we'll evaluate!)\n",
    "- **published**: When it was submitted to arXiv\n",
    "- **url**: Direct link to the paper\n",
    "\n",
    "Now the question is: **which of these 20 papers should our team actually read?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Batch Evaluation with AI\n",
    "\n",
    "Now we'll use an LLM to evaluate each paper's relevance to our team.\n",
    "\n",
    "### The Team Profile\n",
    "\n",
    "First, we define what our team cares about. This is the \"context\" the AI uses to judge relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Focus:\n",
      "The team is building AI-powered applications and wants to stay\n",
      "        current with the latest research on language models, inference\n",
      "        optimization, and practical AI engineering.\n",
      "\n",
      "Interests:\n",
      "  â€¢ Large language model architectures and improvements\n",
      "  â€¢ Inference optimization and cost reduction\n",
      "  â€¢ Prompt engineering and techniques\n",
      "  â€¢ AI agents and tool use\n",
      "  â€¢ Evaluation methods for LLMs\n"
     ]
    }
   ],
   "source": [
    "# Define what makes a paper \"relevant\" for our team\n",
    "# Try modifying this to see how it changes the results!\n",
    "\n",
    "TEAM_PROFILE = {\n",
    "    \"focus\": \"\"\"\n",
    "        The team is building AI-powered applications and wants to stay\n",
    "        current with the latest research on language models, inference\n",
    "        optimization, and practical AI engineering.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"interests\": [\n",
    "        \"Large language model architectures and improvements\",\n",
    "        \"Inference optimization and cost reduction\",\n",
    "        \"Prompt engineering and techniques\",\n",
    "        \"AI agents and tool use\",\n",
    "        \"Evaluation methods for LLMs\",\n",
    "    ],\n",
    "    \n",
    "    \"avoid\": [\n",
    "        \"Pure theoretical papers without practical applications\",\n",
    "        \"Incremental benchmark improvements\",\n",
    "        \"Papers focused only on training from scratch\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Team Focus:\")\n",
    "print(TEAM_PROFILE['focus'].strip())\n",
    "print(\"\\nInterests:\")\n",
    "for interest in TEAM_PROFILE['interests']:\n",
    "    print(f\"  â€¢ {interest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evaluation Prompt\n",
    "\n",
    "This is the prompt we send to the LLM for each paper.\n",
    "Notice how we:\n",
    "1. Give context about the team\n",
    "2. Provide the paper details\n",
    "3. Ask for structured JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ SAMPLE PROMPT (first paper):\n",
      "================================================================================\n",
      "You are evaluating research papers for an AI engineering team.\n",
      "\n",
      "TEAM PROFILE:\n",
      "The team is building AI-powered applications and wants to stay\n",
      "        current with the latest research on language models, inference\n",
      "        optimization, and practical AI engineering.\n",
      "\n",
      "What they find valuable:\n",
      "  - Large language model architectures and improvements\n",
      "  - Inference optimization and cost reduction\n",
      "  - Prompt engineering and techniques\n",
      "  - AI agents and tool use\n",
      "  - Evaluation methods for LLMs\n",
      "\n",
      "What to avoid recommending:\n",
      "  - Pure theoretical papers without practical applications\n",
      "  - Incremental benchmark improvements\n",
      "  - Papers focused only on training from scratch\n",
      "\n",
      "---\n",
      "\n",
      "PAPER TO EVALUATE:\n",
      "\n",
      "Title: Efficient Inference for Large Language Models: A Survey\n",
      "\n",
      "Abstract:\n",
      "Large Language Models (LLMs) have revolutionized natural language processing but face significant challenges in deployment due to their computational requirements. This survey comprehensively reviews recent advances in efficient LLM inference, covering techniques such as quantization, pruning, knowledge distillation, and speculative decoding. We analyze the trade-offs between efficiency and model quality, providing practitioners with actionable guidance for deploying LLMs in resource-constrained environments. Our analysis covers over 100 recent papers and includes benchmarks across various model sizes and hardware configurations.\n",
      "\n",
      "---\n",
      "\n",
      "INSTRUCTIONS:\n",
      "Score this paper's relevance to the team on a scale of 0-10.\n",
      "- 0-3: Not relev...\n"
     ]
    }
   ],
   "source": [
    "def create_evaluation_prompt(paper: dict, team_profile: dict) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt that asks the AI to evaluate a paper.\n",
    "    \n",
    "    This is where the \"magic\" happens - we give the model:\n",
    "    1. Context about what the team cares about\n",
    "    2. The paper's title and abstract\n",
    "    3. Clear instructions on how to respond\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format interests and avoid lists as bullet points\n",
    "    interests_text = \"\\n\".join(f\"  - {item}\" for item in team_profile['interests'])\n",
    "    avoid_text = \"\\n\".join(f\"  - {item}\" for item in team_profile['avoid'])\n",
    "    \n",
    "    prompt = f\"\"\"You are evaluating research papers for an AI engineering team.\n",
    "\n",
    "TEAM PROFILE:\n",
    "{team_profile['focus'].strip()}\n",
    "\n",
    "What they find valuable:\n",
    "{interests_text}\n",
    "\n",
    "What to avoid recommending:\n",
    "{avoid_text}\n",
    "\n",
    "---\n",
    "\n",
    "PAPER TO EVALUATE:\n",
    "\n",
    "Title: {paper['title']}\n",
    "\n",
    "Abstract:\n",
    "{paper['abstract']}\n",
    "\n",
    "---\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Score this paper's relevance to the team on a scale of 0-10.\n",
    "- 0-3: Not relevant\n",
    "- 4-6: Somewhat relevant\n",
    "- 7-10: Highly relevant, team should read this\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{{\n",
    "    \"relevance_score\": <integer 0-10>,\n",
    "    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n",
    "    \"why_relevant\": \"<one sentence explaining why this score>\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Let's see what a prompt looks like\n",
    "sample_prompt = create_evaluation_prompt(papers[0], TEAM_PROFILE)\n",
    "print(\"ðŸ“ SAMPLE PROMPT (first paper):\")\n",
    "print(\"=\"*80)\n",
    "print(sample_prompt[:1500] + \"...\" if len(sample_prompt) > 1500 else sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Batch File\n",
    "\n",
    "The Batch API expects a JSONL file (one JSON object per line).\n",
    "Each line represents one request to the API. We can either build the JSONl file using `create_batch_file` below or we can use [autobatcher](https://docs.doubleword.ai/batches/autobatcher)\n",
    "\n",
    "#### Option A: Create Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created batch file: batch_input.jsonl\n",
      "  Contains 20 requests\n"
     ]
    }
   ],
   "source": [
    "def create_batch_file(papers: list[dict], model: str, output_path: str = \"batch_input.jsonl\") -> str:\n",
    "    \"\"\"\n",
    "    Create a JSONL file for the Batch API.\n",
    "    \n",
    "    Each line contains:\n",
    "    - custom_id: Links response back to the paper\n",
    "    - method: HTTP method (POST)\n",
    "    - url: API endpoint\n",
    "    - body: The actual request (model, messages, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for paper in papers:\n",
    "            request = {\n",
    "                \"custom_id\": paper['id'],  # We'll use this to match responses\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": create_evaluation_prompt(paper, TEAM_PROFILE)\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create the batch file\n",
    "batch_file = create_batch_file(papers, MODEL_NAME)\n",
    "print(f\"âœ“ Created batch file: {batch_file}\")\n",
    "print(f\"  Contains {len(papers)} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ First 2 lines of batch file:\n",
      "================================================================================\n",
      "\n",
      "Request 1:\n",
      "  custom_id: 2401.00001v1\n",
      "  method: POST\n",
      "  model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "  message length: 1799 chars\n",
      "\n",
      "Request 2:\n",
      "  custom_id: 2401.00002v1\n",
      "  method: POST\n",
      "  model: Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\n",
      "  message length: 1736 chars\n"
     ]
    }
   ],
   "source": [
    "# Let's peek at the batch file\n",
    "print(\"ðŸ“„ First 2 lines of batch file:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open(batch_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 2:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # Show a truncated version\n",
    "        print(f\"\\nRequest {i+1}:\")\n",
    "        print(f\"  custom_id: {data['custom_id']}\")\n",
    "        print(f\"  method: {data['method']}\")\n",
    "        print(f\"  model: {data['body']['model']}\")\n",
    "        print(f\"  message length: {len(data['body']['messages'][0]['content'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to the Batch API\n",
    "\n",
    "Now we:\n",
    "1. Upload the batch file\n",
    "2. Create a batch job\n",
    "3. Wait for completion\n",
    "4. Download results\n",
    "\n",
    "**Note:** This requires a valid API key. If you don't have one, skip to the \"Mock Results\" section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_batch(client: OpenAI, input_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload file and create a batch job.\n",
    "    Returns the batch ID for tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Upload the file\n",
    "    print(\"ðŸ“¤ Uploading batch file...\")\n",
    "    with open(input_file, 'rb') as f:\n",
    "        uploaded_file = client.files.create(file=f, purpose=\"batch\")\n",
    "    print(f\"   File ID: {uploaded_file.id}\")\n",
    "    \n",
    "    # Step 2: Create the batch job\n",
    "    print(\"ðŸš€ Creating batch job...\")\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=uploaded_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\" # this can be 24hr or 1hr \n",
    "    )\n",
    "    print(f\"   Batch ID: {batch.id}\")\n",
    "    print(f\"   Status: {batch.status}\")\n",
    "    \n",
    "    return batch.id\n",
    "\n",
    "\n",
    "def wait_for_batch(client: OpenAI, batch_id: str, poll_interval: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Poll until batch completes, then return output file ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nâ³ Waiting for batch to complete...\")\n",
    "    print(\"   (This typically takes 1-5 minutes)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        status = batch.status\n",
    "        \n",
    "        # Show progress\n",
    "        completed = (batch.request_counts.completed or 0)\n",
    "        failed = (batch.request_counts.failed or 0)\n",
    "        total = batch.request_counts.total or 0\n",
    "        \n",
    "        print(f\"   Status: {status} | Progress: {completed + failed}/{total}\")\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            print(\"\\nâœ“ Batch completed!\")\n",
    "            return batch.output_file_id\n",
    "        elif status in [\"failed\", \"expired\", \"cancelled\"]:\n",
    "            raise Exception(f\"Batch failed with status: {status}\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def download_results(client: OpenAI, output_file_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Download and parse the batch results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ“¥ Downloading results...\")\n",
    "    content = client.files.content(output_file_id)\n",
    "    \n",
    "    results = []\n",
    "    for line in content.text.strip().split('\\n'):\n",
    "        if line:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    print(f\"   Downloaded {len(results)} results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Uploading batch file...\n",
      "   File ID: e6e3dd08-933b-4ff4-b303-c567e5ba7eac\n",
      "ðŸš€ Creating batch job...\n",
      "   Batch ID: f2f43151-5ee3-4668-ad57-72f514119beb\n",
      "   Status: in_progress\n",
      "\n",
      "â³ Waiting for batch to complete...\n",
      "   (This typically takes 1-5 minutes)\n",
      "\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n",
      "   Status: in_progress | Progress: 0/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Submit and wait\u001b[39;00m\n\u001b[32m     14\u001b[39m batch_id = submit_batch(client, batch_file)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m output_file_id = \u001b[43mwait_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m raw_results = download_results(client, output_file_id)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Batch evaluation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mwait_for_batch\u001b[39m\u001b[34m(client, batch_id, poll_interval)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexpired\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcancelled\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch failed with status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m time.sleep(poll_interval)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the batch evaluation\n",
    "# Set this to True when you have a valid API key\n",
    "RUN_BATCH_API = True\n",
    "\n",
    "if RUN_BATCH_API and API_KEY:\n",
    "    # Create the client\n",
    "    client_kwargs = {'api_key': API_KEY}\n",
    "    if BASE_URL:\n",
    "        client_kwargs['base_url'] = BASE_URL\n",
    "    \n",
    "    client = OpenAI(**client_kwargs)\n",
    "       \n",
    "    # Submit and wait\n",
    "    batch_id = submit_batch(client, batch_file)\n",
    "    output_file_id = wait_for_batch(client, batch_id)\n",
    "    raw_results = download_results(client, output_file_id)\n",
    "    \n",
    "    print(\"\\nâœ“ Batch evaluation complete!\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping live API call (RUN_BATCH_API = False)\")\n",
    "    print(\"   We'll use mock results in the next cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Use AutoBatcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:59:22.830\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mInitialized with batch_size=20, window=60.0s\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:22.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_enqueue_request\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mStarting 60.0s batch window timer\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:22.832\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_enqueue_request\u001b[0m:\u001b[36m208\u001b[0m - \u001b[34m\u001b[1mBatch size 20 reached\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting batch evaluation with autobatcher...\n",
      "ðŸ“¤ Sending 20 papers to autobatcher (batch_size=20)...\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25% | Preparing batch (20 requests)...\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50% | Uploading file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:59:23.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_submit_batch\u001b[0m:\u001b[36m277\u001b[0m - \u001b[34m\u001b[1mUploaded batch file: a94cc72f-5825-4ac3-aeb1-ae7cb6160278\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  75% | Creating batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:59:24.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_submit_batch\u001b[0m:\u001b[36m287\u001b[0m - \u001b[34m\u001b[1mSubmitted batch d64e42bb-9f59-43ab-afc3-b0742af84bc2 with 20 requests\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:24.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m318\u001b[0m - \u001b[34m\u001b[1mPoller started with 1 active batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% | Submitted âœ“ (batch d64e42bb-9f5...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:59:29.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mBatch d64e42bb-9f5 status: in_progress (completed=0/20)\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:35.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mBatch d64e42bb-9f5 status: in_progress (completed=0/20)\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:42.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mBatch d64e42bb-9f5 status: in_progress (completed=0/20)\u001b[0m\n",
      "\u001b[32m2026-02-03 11:59:48.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mautobatcher.client\u001b[0m:\u001b[36m_poll_batches\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mBatch d64e42bb-9f5 status: in_progress (completed=0/20)\u001b[0m\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Starting batch evaluation with autobatcher...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m evaluations = \u001b[38;5;28;01mawait\u001b[39;00m evaluate_papers_with_autobatcher(papers, MODEL_NAME, TEAM_PROFILE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mevaluate_papers_with_autobatcher\u001b[39m\u001b[34m(papers, model, team_profile)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¤ Sending \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m papers to autobatcher (batch_size=20)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m tasks = [evaluate_single_paper(paper) \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Filter out any None results\u001b[39;00m\n\u001b[32m     41\u001b[39m evaluations = [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mevaluate_papers_with_autobatcher.<locals>.evaluate_single_paper\u001b[39m\u001b[34m(paper)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mevaluate_single_paper\u001b[39m(paper):\n\u001b[32m     23\u001b[39m     prompt = create_evaluation_prompt(paper, team_profile)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m     25\u001b[39m         model=model,\n\u001b[32m     26\u001b[39m         max_tokens=\u001b[32m500\u001b[39m,\n\u001b[32m     27\u001b[39m         messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m     28\u001b[39m     )\n\u001b[32m     29\u001b[39m     content = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     30\u001b[39m     evaluation = parse_json_response(content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/autobatcher/src/autobatcher/client.py:79\u001b[39m, in \u001b[36m_ChatCompletions.create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcreate\u001b[39m(\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     69\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     **kwargs: Any,\n\u001b[32m     73\u001b[39m ) -> ChatCompletion:\n\u001b[32m     74\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33;03m    Create a chat completion. The request is queued and batched.\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33;03m    Returns when the batch completes and results are available.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client._enqueue_request(\n\u001b[32m     80\u001b[39m         model=model,\n\u001b[32m     81\u001b[39m         messages=messages,\n\u001b[32m     82\u001b[39m         **kwargs,\n\u001b[32m     83\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/autobatcher/src/autobatcher/client.py:211\u001b[39m, in \u001b[36mBatchOpenAI._enqueue_request\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mBatch size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m reached\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._batch_size)\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._submit_batch()\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Using autobatcher for efficient batch processing\n",
    "# Install: pip install autobatcher\n",
    "\n",
    "import asyncio\n",
    "from autobatcher import BatchOpenAI\n",
    "\n",
    "async def evaluate_papers_with_autobatcher(papers, model, team_profile):\n",
    "    \"\"\"\n",
    "    Evaluate papers using autobatcher - waits for 20 requests before sending.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create autobatcher client - batch_size=20 means it waits for 20 requests\n",
    "    async with BatchOpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "        batch_size=20,              # Wait until we have 20 requests\n",
    "        batch_window_seconds=60.0,  # Or send after 60 seconds if we don't hit 20\n",
    "        poll_interval_seconds=5.0,  # Check for results every 5 seconds\n",
    "    ) as client:\n",
    "        \n",
    "        # Create all evaluation tasks concurrently\n",
    "        async def evaluate_single_paper(paper):\n",
    "            prompt = create_evaluation_prompt(paper, team_profile)\n",
    "            response = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                max_tokens=500,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            evaluation = parse_json_response(content)\n",
    "            if evaluation:\n",
    "                evaluation['paper_id'] = paper['id']\n",
    "            return evaluation\n",
    "        \n",
    "        # Send all 20 papers at once - autobatcher will batch them\n",
    "        print(f\"ðŸ“¤ Sending {len(papers)} papers to autobatcher (batch_size=20)...\")\n",
    "        tasks = [evaluate_single_paper(paper) for paper in papers]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Filter out any None results\n",
    "        evaluations = [r for r in results if r is not None]\n",
    "        print(f\"âœ“ Received {len(evaluations)} evaluations\")\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "# Run the evaluation\n",
    "print(\"ðŸš€ Starting batch evaluation with autobatcher...\")\n",
    "evaluations = await evaluate_papers_with_autobatcher(papers, MODEL_NAME, TEAM_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Results (For Workshop Demo)\n",
    "\n",
    "If you didn't run the actual API call, here are realistic mock results\n",
    "so you can see how the rest of the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mock results that simulate what the API would return\n# These are pre-generated evaluations for our sample papers (REAL arXiv papers)\n\nMOCK_EVALUATIONS = [\n    {\"paper_id\": \"2410.21276v1\", \"relevance_score\": 10, \n     \"key_insight\": \"Comprehensive survey of tool learning for LLM-based agents covering understanding, creation, and optimization.\",\n     \"why_relevant\": \"Directly addresses AI agents and tool use - a primary team interest.\"},\n    \n    {\"paper_id\": \"2310.04406v2\", \"relevance_score\": 9,\n     \"key_insight\": \"AgentTuning improves LLM agent abilities while maintaining general capabilities through hybrid instruction-tuning.\",\n     \"why_relevant\": \"Practical approach to building better AI agents.\"},\n    \n    {\"paper_id\": \"2312.10997v3\", \"relevance_score\": 7,\n     \"key_insight\": \"Framework for governing agentic AI systems with practical deployment guidelines.\",\n     \"why_relevant\": \"Relevant for safe deployment but more governance-focused than technical.\"},\n    \n    {\"paper_id\": \"2401.04088v2\", \"relevance_score\": 9,\n     \"key_insight\": \"Mixtral 8x7B achieves strong performance with 6x faster inference using sparse mixture of experts.\",\n     \"why_relevant\": \"Directly addresses inference optimization - core team interest.\"},\n    \n    {\"paper_id\": \"2403.03507v2\", \"relevance_score\": 8,\n     \"key_insight\": \"Claude 3 family shows strong multimodal performance with reduced incorrect refusals.\",\n     \"why_relevant\": \"Important benchmark for frontier model capabilities.\"},\n    \n    {\"paper_id\": \"2305.10601v2\", \"relevance_score\": 9,\n     \"key_insight\": \"Tree of Thoughts enables deliberate problem-solving through exploration and backtracking.\",\n     \"why_relevant\": \"Key prompting technique for complex reasoning tasks.\"},\n    \n    {\"paper_id\": \"2210.03629v3\", \"relevance_score\": 10,\n     \"key_insight\": \"ReAct synergizes reasoning and acting by interleaving thought traces with actions.\",\n     \"why_relevant\": \"Foundational paper for AI agents - essential reading.\"},\n    \n    {\"paper_id\": \"2305.14314v2\", \"relevance_score\": 9,\n     \"key_insight\": \"QLoRA enables 65B model fine-tuning on single GPU with 4-bit quantization and LoRA.\",\n     \"why_relevant\": \"Critical for efficient fine-tuning and deployment.\"},\n    \n    {\"paper_id\": \"2307.09288v2\", \"relevance_score\": 8,\n     \"key_insight\": \"Llama 2 provides strong open-source foundation models up to 70B parameters.\",\n     \"why_relevant\": \"Important baseline model for the ecosystem.\"},\n    \n    {\"paper_id\": \"2309.16609v2\", \"relevance_score\": 9,\n     \"key_insight\": \"Mistral 7B outperforms larger models with efficient architecture design.\",\n     \"why_relevant\": \"Demonstrates inference efficiency gains - core interest.\"},\n    \n    {\"paper_id\": \"2305.18290v2\", \"relevance_score\": 8,\n     \"key_insight\": \"DPO simplifies preference learning without explicit reward modeling or RL.\",\n     \"why_relevant\": \"Practical fine-tuning technique, though more training-focused.\"},\n    \n    {\"paper_id\": \"2402.03300v2\", \"relevance_score\": 9,\n     \"key_insight\": \"Self-Discover enables LLMs to compose their own reasoning structures for complex tasks.\",\n     \"why_relevant\": \"Advanced prompting technique for reasoning improvements.\"},\n    \n    {\"paper_id\": \"2310.06825v2\", \"relevance_score\": 9,\n     \"key_insight\": \"Self-RAG trains models to adaptively retrieve and self-critique for better factuality.\",\n     \"why_relevant\": \"Practical approach to reducing hallucinations in production.\"},\n    \n    {\"paper_id\": \"2401.02954v2\", \"relevance_score\": 7,\n     \"key_insight\": \"DeepSeek LLM provides transparent open-source models up to 67B parameters.\",\n     \"why_relevant\": \"Good open-source option but less novel techniques.\"},\n    \n    {\"paper_id\": \"2308.12950v3\", \"relevance_score\": 9,\n     \"key_insight\": \"Code Llama provides state-of-the-art open code generation with 100K context.\",\n     \"why_relevant\": \"Directly applicable to AI-assisted coding workflows.\"},\n    \n    {\"paper_id\": \"2312.11805v3\", \"relevance_score\": 8,\n     \"key_insight\": \"Gemini demonstrates strong multimodal capabilities across text, image, audio, video.\",\n     \"why_relevant\": \"Important frontier model benchmark.\"},\n    \n    {\"paper_id\": \"2310.12931v2\", \"relevance_score\": 7,\n     \"key_insight\": \"Llemma excels at mathematical reasoning and formal theorem proving.\",\n     \"why_relevant\": \"Specialized model, less general applicability.\"},\n    \n    {\"paper_id\": \"2309.05463v2\", \"relevance_score\": 8,\n     \"key_insight\": \"phi-1.5 shows data quality matters more than quantity for small models.\",\n     \"why_relevant\": \"Relevant insight for efficient model training.\"},\n    \n    {\"paper_id\": \"2401.10020v1\", \"relevance_score\": 10,\n     \"key_insight\": \"Comprehensive survey of resource-efficient techniques including compression and optimization.\",\n     \"why_relevant\": \"Essential reading for inference optimization goals.\"},\n    \n    {\"paper_id\": \"2403.08295v2\", \"relevance_score\": 8,\n     \"key_insight\": \"SIMA demonstrates generalist agents that follow instructions across game environments.\",\n     \"why_relevant\": \"Interesting for agent generalization, though game-focused.\"}\n]\n\n# Use mock results if we didn't run the API\nif not RUN_BATCH_API or not API_KEY:\n    evaluations = MOCK_EVALUATIONS\n    print(f\"âœ“ Using {len(evaluations)} mock evaluations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Results\n",
    "\n",
    "If you ran the real API, we need to parse the JSON responses.\n",
    "The mock results are already in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(content: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Extract JSON from the model's response.\n",
    "    Handles cases where the model includes extra text.\n",
    "    \"\"\"\n",
    "    content = content.strip()\n",
    "    \n",
    "    # Try direct parsing first\n",
    "    if content.startswith('{'):\n",
    "        try:\n",
    "            return json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Try to find JSON anywhere in the response\n",
    "    json_match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_batch_results(raw_results: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse raw API results into evaluation dictionaries.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    \n",
    "    for result in raw_results:\n",
    "        paper_id = result['custom_id']\n",
    "        \n",
    "        # Check for errors\n",
    "        if result.get('error'):\n",
    "            print(f\"  âš ï¸ Error for {paper_id}: {result['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract content from OpenAI response format\n",
    "        response_body = result.get('response', {}).get('body', {})\n",
    "        choices = response_body.get('choices', [])\n",
    "        \n",
    "        if not choices:\n",
    "            continue\n",
    "        \n",
    "        content = choices[0].get('message', {}).get('content', '')\n",
    "        evaluation = parse_json_response(content)\n",
    "        \n",
    "        if evaluation:\n",
    "            evaluation['paper_id'] = paper_id\n",
    "            evaluations.append(evaluation)\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "# Parse real results if we have them\n",
    "if RUN_BATCH_API and API_KEY:\n",
    "    evaluations = parse_batch_results(raw_results)\n",
    "    print(f\"âœ“ Parsed {len(evaluations)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Results with Paper Data\n",
    "\n",
    "Now let's combine the evaluations with the original paper data\n",
    "so we have everything in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged 20 papers with evaluations\n"
     ]
    }
   ],
   "source": [
    "def merge_results_with_papers(papers: list[dict], evaluations: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Combine paper data with evaluation results.\n",
    "    Returns merged list sorted by relevance score (highest first).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create lookup for evaluations by paper_id\n",
    "    eval_lookup = {e['paper_id']: e for e in evaluations}\n",
    "    \n",
    "    merged = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        if paper_id in eval_lookup:\n",
    "            # Combine paper data with evaluation\n",
    "            combined = {**paper, **eval_lookup[paper_id]}\n",
    "            combined.pop('paper_id', None)  # Remove redundant field\n",
    "            merged.append(combined)\n",
    "    \n",
    "    # Sort by relevance score (highest first)\n",
    "    merged.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Merge everything together\n",
    "results = merge_results_with_papers(papers, MOCK_EVALUATIONS)\n",
    "print(f\"âœ“ Merged {len(results)} papers with evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top 10 Papers\n",
    "\n",
    "Finally! Let's see which papers the AI thinks our team should read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 10 PAPERS FOR YOUR TEAM (score >= 7)\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¥ [10/10] ReAct Agents: Combining Reasoning and Acting in Language Models\n",
      "   ðŸ’¡ Improved framework for AI agents that interleave reasoning with actions for better decision-making.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00002\n",
      "\n",
      "ðŸ”¥ [10/10] Tool Use in Large Language Models: A Comprehensive Survey\n",
      "   ðŸ’¡ Comprehensive survey of tool-augmented LLMs covering selection, invocation, and error recovery.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00005\n",
      "\n",
      "ðŸ”¥ [10/10] Deployment Patterns for LLM Applications in Production\n",
      "   ðŸ’¡ Catalog of deployment patterns for LLM applications covering batching, caching, and monitoring.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00019\n",
      "\n",
      "ðŸ”¥ [9/10] Efficient Inference for Large Language Models: A Survey\n",
      "   ðŸ’¡ Comprehensive survey of inference optimization techniques including quantization, pruning, and speculative decoding.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00001\n",
      "\n",
      "ðŸ”¥ [9/10] Prompt Engineering Best Practices for Production Systems\n",
      "   ðŸ’¡ Systematic study of prompt engineering across 15 enterprise use cases with 23% accuracy improvement.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00003\n",
      "\n",
      "ðŸ”¥ [9/10] Multi-Agent Collaboration with Large Language Models\n",
      "   ðŸ’¡ Multi-agent framework with specialized agents outperforms single-agent by 34% on software tasks.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00008\n",
      "\n",
      "ðŸ”¥ [9/10] Quantization Techniques for Efficient LLM Deployment\n",
      "   ðŸ’¡ AdaptiveQuant achieves 4-bit quantization with less than 1% accuracy loss.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00009\n",
      "\n",
      "ðŸ”¥ [9/10] Code Generation with Large Language Models: A Practical Guide\n",
      "   ðŸ’¡ Practical guide for deploying LLM code generation with security and integration best practices.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00012\n",
      "\n",
      "ðŸ”¥ [9/10] Speculative Decoding: Accelerating LLM Inference Without Quality ...\n",
      "   ðŸ’¡ Speculative decoding achieves 2-3x inference speedup with no quality degradation.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00013\n",
      "\n",
      "â­ [8/10] Retrieval-Augmented Generation: Reducing Hallucinations in LLMs\n",
      "   ðŸ’¡ RAG-Verify reduces hallucinations by 67% through retrieval and verification modules.\n",
      "   ðŸ“Ž https://arxiv.org/abs/2401.00006\n",
      "\n",
      "================================================================================\n",
      "Showing 11 papers with relevance score >= 7\n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 most relevant papers\n",
    "TOP_N = 10\n",
    "MIN_SCORE = 7  # Only show papers with score >= 7\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {TOP_N} PAPERS FOR YOUR TEAM (score >= {MIN_SCORE})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shown = 0\n",
    "for paper in results:\n",
    "    score = paper.get('relevance_score', 0)\n",
    "    \n",
    "    if score < MIN_SCORE:\n",
    "        continue\n",
    "    \n",
    "    shown += 1\n",
    "    if shown > TOP_N:\n",
    "        break\n",
    "    \n",
    "    # Score indicator\n",
    "    if score >= 9:\n",
    "        indicator = \"ðŸ”¥\"\n",
    "    elif score >= 7:\n",
    "        indicator = \"â­\"\n",
    "    else:\n",
    "        indicator = \"ðŸ“„\"\n",
    "    \n",
    "    print(f\"\\n{indicator} [{score}/10] {paper['title'][:65]}{'...' if len(paper['title']) > 65 else ''}\")\n",
    "    print(f\"   ðŸ’¡ {paper.get('key_insight', 'N/A')}\")\n",
    "    print(f\"   ðŸ“Ž {paper['url']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Showing {shown} papers with relevance score >= {MIN_SCORE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Understanding the Results\n",
    "\n",
    "Let's analyze what we got and discuss the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The JSON Structure\n",
    "\n",
    "Each evaluated paper now contains both the original arXiv data\n",
    "and the AI-generated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Complete structure of a top-rated paper:\n",
      "================================================================================\n",
      "{\n",
      "  \"id\": \"2401.00002v1\",\n",
      "  \"title\": \"ReAct Agents: Combining Reasoning and Acting in La...\",\n",
      "  \"authors\": [\n",
      "    \"David Park\",\n",
      "    \"Emily Zhang\"\n",
      "  ],\n",
      "  \"abstract\": \"We present an improved framework for building AI agents that interleave reasoning traces with action...\",\n",
      "  \"published\": \"2024-01-14T10:30:00Z\",\n",
      "  \"url\": \"https://arxiv.org/abs/2401.00002\",\n",
      "  \"relevance_score\": 10,\n",
      "  \"key_insight\": \"Improved framework for AI agents that interleave reasoning with actions for better decision-making.\",\n",
      "  \"why_relevant\": \"Highly relevant - AI agents and tool use is a primary team interest.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Look at the complete structure of a top paper\n",
    "print(\"ðŸ“Š Complete structure of a top-rated paper:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_paper = results[0]\n",
    "print(json.dumps({\n",
    "    'id': top_paper['id'],\n",
    "    'title': top_paper['title'][:50] + '...',\n",
    "    'authors': top_paper['authors'][:2],\n",
    "    'abstract': top_paper['abstract'][:100] + '...',\n",
    "    'published': top_paper['published'],\n",
    "    'url': top_paper['url'],\n",
    "    'relevance_score': top_paper['relevance_score'],\n",
    "    'key_insight': top_paper['key_insight'],\n",
    "    'why_relevant': top_paper['why_relevant']\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Distribution\n",
    "\n",
    "Let's see how the scores are distributed across all papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Score Distribution:\n",
      "========================================\n",
      "High (9-10)     | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (9)\n",
      "Good (7-8)      | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (7)\n",
      "Medium (4-6)    | â–ˆâ–ˆâ–ˆâ–ˆ (4)\n",
      "Low (0-3)       |  (0)\n",
      "\n",
      "Average score: 7.8\n"
     ]
    }
   ],
   "source": [
    "# Count papers in each score range\n",
    "score_ranges = {\n",
    "    'High (9-10)': 0,\n",
    "    'Good (7-8)': 0,\n",
    "    'Medium (4-6)': 0,\n",
    "    'Low (0-3)': 0\n",
    "}\n",
    "\n",
    "for paper in results:\n",
    "    score = paper.get('relevance_score', 0)\n",
    "    if score >= 9:\n",
    "        score_ranges['High (9-10)'] += 1\n",
    "    elif score >= 7:\n",
    "        score_ranges['Good (7-8)'] += 1\n",
    "    elif score >= 4:\n",
    "        score_ranges['Medium (4-6)'] += 1\n",
    "    else:\n",
    "        score_ranges['Low (0-3)'] += 1\n",
    "\n",
    "print(\"ðŸ“ˆ Score Distribution:\")\n",
    "print(\"=\"*40)\n",
    "for range_name, count in score_ranges.items():\n",
    "    bar = \"â–ˆ\" * count\n",
    "    print(f\"{range_name:15} | {bar} ({count})\")\n",
    "\n",
    "# Average score\n",
    "avg_score = sum(p.get('relevance_score', 0) for p in results) / len(results)\n",
    "print(f\"\\nAverage score: {avg_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Score Distribution Problem\n",
    "\n",
    "Notice the average score is **7.8** - and most papers cluster between 7-9. This is a common problem with LLM evaluation!\n",
    "\n",
    "The model is being \"polite\" and hedging - everything seems \"pretty good.\" With such low variance, it's hard to prioritize which papers to actually read.\n",
    "\n",
    "**We need external signals to differentiate!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Adding External Signals (Citations)\n",
    "\n",
    "The LLM gives us *relevance* to our team, but it's not the only signal. **Citation counts** tell us how the broader research community values each paper.\n",
    "\n",
    "**Important for recent papers:** Since we're looking at papers from the last 7 days, citation counts will be LOW (typically 0-50). Even 10+ citations in a week indicates a paper is getting attention!\n",
    "\n",
    "| LLM Score | Citations | Interpretation |\n",
    "|-----------|-----------|----------------|\n",
    "| High | High (for age) | **Must read** - relevant AND gaining traction |\n",
    "| High | Low | New/niche - relevant but hasn't spread yet |\n",
    "| Low | High | Important to others, but not for us |\n",
    "| Low | Low | Skip |\n",
    "\n",
    "Let's add citation data using Semantic Scholar's free API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Fetching citation data from Semantic Scholar...\n",
      "  [1/20] Checking: ReAct Agents: Combining Reasoning and Acting in La...\n",
      "  [2/20] Checking: Tool Use in Large Language Models: A Comprehensive...\n",
      "  [3/20] Checking: Deployment Patterns for LLM Applications in Produc...\n",
      "  [4/20] Checking: Efficient Inference for Large Language Models: A S...\n",
      "  [5/20] Checking: Prompt Engineering Best Practices for Production S...\n",
      "  [6/20] Checking: Multi-Agent Collaboration with Large Language Mode...\n",
      "  [7/20] Checking: Quantization Techniques for Efficient LLM Deployme...\n",
      "  [8/20] Checking: Code Generation with Large Language Models: A Prac...\n",
      "  [9/20] Checking: Speculative Decoding: Accelerating LLM Inference W...\n",
      "  [10/20] Checking: Retrieval-Augmented Generation: Reducing Hallucina...\n",
      "  [11/20] Checking: Evaluating Large Language Models: Challenges and B...\n",
      "  [12/20] Checking: Instruction Following in Large Language Models...\n",
      "  [13/20] Checking: In-Context Learning: Understanding How LLMs Learn ...\n",
      "  [14/20] Checking: Constitutional AI: Alignment through Self-Improvem...\n",
      "  [15/20] Checking: Long Context Understanding in Transformer Models...\n",
      "  [16/20] Checking: Mixture of Experts for Efficient Large Language Mo...\n",
      "  [17/20] Checking: Safety Alignment in Large Language Models: Methods...\n",
      "  [18/20] Checking: Scaling Laws for Fine-tuning Language Models...\n",
      "  [19/20] Checking: Advanced Topics in Transformer Architecture Design...\n",
      "  [20/20] Checking: Theory of Mind in Large Language Models: An Empiri...\n",
      "\n",
      "âœ“ Found citation data for 16/20 papers\n"
     ]
    }
   ],
   "source": [
    "# Import our tools for citation lookup\n",
    "from tools import check_citation_impact, get_citations_batch, create_composite_score\n",
    "\n",
    "# For workshop demo, we'll use pre-computed citation data\n",
    "# (Real API calls take ~10 seconds for 20 papers due to rate limiting)\n",
    "\n",
    "USE_LIVE_CITATIONS = True  # Set True to fetch real citation data\n",
    "\n",
    "if USE_LIVE_CITATIONS:\n",
    "    print(\"ðŸ“š Fetching citation data from Semantic Scholar...\")\n",
    "    results_with_citations = get_citations_batch(results)\n",
    "else:\n",
    "    # Mock citation data - REALISTIC for recent papers (last 7 days)\n",
    "    # New papers typically have 0-50 citations, not hundreds!\n",
    "    MOCK_CITATIONS = {\n",
    "        \"2401.00001v1\": 28,    # Survey - getting attention\n",
    "        \"2401.00002v1\": 45,    # ReAct follow-up - popular topic\n",
    "        \"2401.00003v1\": 12,    # Practical guide\n",
    "        \"2401.00004v1\": 8,     # Scaling laws\n",
    "        \"2401.00005v1\": 33,    # Tool use - hot topic\n",
    "        \"2401.00006v1\": 19,    # RAG improvements\n",
    "        \"2401.00007v1\": 15,    # Constitutional AI\n",
    "        \"2401.00008v1\": 22,    # Multi-agent\n",
    "        \"2401.00009v1\": 31,    # Quantization - practical\n",
    "        \"2401.00010v1\": 9,     # Evaluation\n",
    "        \"2401.00011v1\": 6,     # Long context\n",
    "        \"2401.00012v1\": 14,    # Code gen guide\n",
    "        \"2401.00013v1\": 38,    # Speculative decoding - efficiency!\n",
    "        \"2401.00014v1\": 7,     # Instruction following\n",
    "        \"2401.00015v1\": 11,    # MoE\n",
    "        \"2401.00016v1\": 3,     # Theory of mind - niche\n",
    "        \"2401.00017v1\": 5,     # Safety alignment\n",
    "        \"2401.00018v1\": 16,    # In-context learning\n",
    "        \"2401.00019v1\": 21,    # Deployment patterns\n",
    "        \"2401.00020v1\": 4,     # Architecture ablation\n",
    "    }\n",
    "    \n",
    "    results_with_citations = []\n",
    "    for paper in results:\n",
    "        paper_with_cit = {**paper}\n",
    "        paper_with_cit['citations'] = MOCK_CITATIONS.get(paper['id'], 0)\n",
    "        paper_with_cit['found'] = True\n",
    "        results_with_citations.append(paper_with_cit)\n",
    "    \n",
    "    print(\"âœ“ Using mock citation data (realistic for recent papers: 0-50 range)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š LLM SCORES vs CITATION COUNTS\n",
      "================================================================================\n",
      "Paper                                         |   LLM |  Citations\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt Engineering Best Practices for Prod... |     9 |         12\n",
      "Mixture of Experts for Efficient Large Lan... |     7 |          9\n",
      "Code Generation with Large Language Models... |     9 |          6\n",
      "Speculative Decoding: Accelerating LLM Inf... |     9 |          3\n",
      "In-Context Learning: Understanding How LLM... |     8 |          3\n",
      "ReAct Agents: Combining Reasoning and Acti... |    10 |          2\n",
      "Tool Use in Large Language Models: A Compr... |    10 |          2\n",
      "Efficient Inference for Large Language Mod... |     9 |          2\n",
      "Multi-Agent Collaboration with Large Langu... |     9 |          2\n",
      "Long Context Understanding in Transformer ... |     7 |          2\n",
      "\n",
      "ðŸ’¡ KEY INSIGHT: Citation counts have MUCH higher variance!\n",
      "   Range: 0 to 12 (vs LLM scores: 4 to 10)\n"
     ]
    }
   ],
   "source": [
    "# Compare LLM scores vs Citation counts\n",
    "print(\"ðŸ“Š LLM SCORES vs CITATION COUNTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Paper':<45} | {'LLM':>5} | {'Citations':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Sort by citations to see the difference\n",
    "sorted_by_citations = sorted(results_with_citations, key=lambda x: x['citations'], reverse=True)\n",
    "\n",
    "for paper in sorted_by_citations[:10]:\n",
    "    title = paper['title'][:42] + \"...\" if len(paper['title']) > 42 else paper['title']\n",
    "    print(f\"{title:<45} | {paper['relevance_score']:>5} | {paper['citations']:>10,}\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ’¡ KEY INSIGHT: Citation counts have MUCH higher variance!\")\n",
    "print(\"   Range: {} to {:,} (vs LLM scores: {} to {})\".format(\n",
    "    min(p['citations'] for p in results_with_citations),\n",
    "    max(p['citations'] for p in results_with_citations),\n",
    "    min(p['relevance_score'] for p in results_with_citations),\n",
    "    max(p['relevance_score'] for p in results_with_citations)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Intelligent Re-ranking with Composite Scores\n",
    "\n",
    "Now let's combine both signals:\n",
    "- **LLM Score** (70%) - Is this relevant to OUR team?\n",
    "- **Citations** (30%) - Does the research community value this?\n",
    "\n",
    "This gives us a composite score that balances **relevance** with **impact**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Auto-detected max_citations=20 based on data)\n",
      "ðŸ“Š PAPERS WITH COMPOSITE SCORES\n",
      "==========================================================================================\n",
      "Paper                                    |   LLM |  Citations |  Composite\n",
      "------------------------------------------------------------------------------------------\n",
      "Prompt Engineering Best Practices for... |     9 |         12 |       8.83\n",
      "Deployment Patterns for LLM Applicati... |    10 |          2 |       8.08\n",
      "Quantization Techniques for Efficient... |     9 |          3 |       7.67\n",
      "Efficient Inference for Large Languag... |     9 |          2 |       7.38\n",
      "Multi-Agent Collaboration with Large ... |     9 |          2 |       7.38\n",
      "ReAct Agents: Combining Reasoning and... |    10 |          0 |       7.00\n",
      "Tool Use in Large Language Models: A ... |    10 |          0 |       7.00\n",
      "Code Generation with Large Language M... |     9 |          0 |       6.30\n",
      "Speculative Decoding: Accelerating LL... |     9 |          0 |       6.30\n",
      "Retrieval-Augmented Generation: Reduc... |     8 |          0 |       5.60\n"
     ]
    }
   ],
   "source": [
    "from tools import add_composite_scores, compare_rankings\n",
    "\n",
    "# Add composite scores (70% LLM, 30% citations)\n",
    "results_with_composite = add_composite_scores(\n",
    "    results_with_citations,\n",
    "    llm_weight=0.7,\n",
    "    citation_weight=0.3\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š PAPERS WITH COMPOSITE SCORES\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Paper':<40} | {'LLM':>5} | {'Citations':>10} | {'Composite':>10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for paper in results_with_composite[:10]:\n",
    "    title = paper['title'][:37] + \"...\" if len(paper['title']) > 37 else paper['title']\n",
    "    print(f\"{title:<40} | {paper['relevance_score']:>5} | {paper['citations']:>10,} | {paper['composite_score']:>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the three ranking strategies side-by-side\n",
    "rankings = compare_rankings(results_with_composite, top_n=10)\n",
    "\n",
    "print(\"\\nðŸ”„ RANKING COMPARISON: How do different strategies rank papers?\")\n",
    "print(\"=\" * 105)\n",
    "print(f\"{'#':<3} | {'LLM-Only Ranking':<32} | {'Citation-Only Ranking':<32} | {'Composite Ranking':<32}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for i in range(5):\n",
    "    llm = rankings['llm_ranking'][i]\n",
    "    cit = rankings['citation_ranking'][i]\n",
    "    comp = rankings['composite_ranking'][i]\n",
    "    \n",
    "    llm_str = f\"{llm['title'][:22]}... (LLM:{llm['relevance_score']})\"\n",
    "    cit_str = f\"{cit['title'][:22]}... ({cit['citations']:,})\"\n",
    "    comp_str = f\"{comp['title'][:22]}... ({comp['composite_score']:.1f})\"\n",
    "    \n",
    "    print(f\"{i+1:<3} | {llm_str:<32} | {cit_str:<32} | {comp_str:<32}\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "print()\n",
    "print(\"ðŸ’¡ Notice how the COMPOSITE ranking surfaces different papers than either alone!\")\n",
    "print(\"   It balances 'relevant to us' with 'valued by the community'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Intelligent Tool Use\n",
    "\n",
    "Now we can make **intelligent decisions** about when to use expensive tools.\n",
    "\n",
    "Instead of downloading PDFs for all papers (slow, wasteful), the agent reasons:\n",
    "> \"This paper is BOTH highly relevant AND highly valued by the community.\n",
    "> It's worth the cost of downloading and analyzing the full PDF.\"\n",
    "\n",
    "This is **agentic behavior** - using judgment to decide when to take action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_paper_pdf\n",
    "\n",
    "# Define thresholds for \"high value\" papers\n",
    "# For RECENT papers (last 7 days), citations will be low (0-50 typically)\n",
    "# Adjust thresholds based on your data!\n",
    "COMPOSITE_THRESHOLD = 8.0   # Top ~20% of papers\n",
    "CITATION_THRESHOLD = 5      # Even 5 citations in a week is notable for new papers\n",
    "\n",
    "print(\"ðŸ¤– AGENT REASONING: Which papers deserve deep investigation?\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Criteria: composite_score >= {COMPOSITE_THRESHOLD} AND citations >= {CITATION_THRESHOLD}\")\n",
    "print(\"(Thresholds adjusted for RECENT papers - new papers have few citations)\")\n",
    "print()\n",
    "\n",
    "high_value_papers = []\n",
    "\n",
    "for paper in results_with_composite:\n",
    "    composite = paper['composite_score']\n",
    "    citations = paper['citations']\n",
    "    llm_score = paper['relevance_score']\n",
    "    \n",
    "    if composite >= COMPOSITE_THRESHOLD and citations >= CITATION_THRESHOLD:\n",
    "        high_value_papers.append(paper)\n",
    "        \n",
    "        print(f\"ðŸ”¥ HIGH VALUE: {paper['title'][:55]}...\")\n",
    "        print(f\"   â””â”€ Composite: {composite:.1f} | LLM: {llm_score} | Citations: {citations:,}\")\n",
    "        print(f\"   â””â”€ Agent reasoning: 'Highly relevant AND gaining traction quickly'\")\n",
    "        print()\n",
    "\n",
    "if not high_value_papers:\n",
    "    print(\"No papers met both thresholds. Try lowering COMPOSITE_THRESHOLD or CITATION_THRESHOLD.\")\n",
    "else:\n",
    "    print(f\"Found {len(high_value_papers)} papers worth deep investigation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally download PDFs for high-value papers\n",
    "DOWNLOAD_PDFS = False  # Set True to actually download\n",
    "\n",
    "if DOWNLOAD_PDFS and high_value_papers:\n",
    "    print(\"ðŸ“¥ Downloading PDFs for high-value papers...\")\n",
    "    for paper in high_value_papers[:3]:  # Limit to top 3\n",
    "        print(f\"ðŸ“„ Downloading: {paper['title'][:50]}...\")\n",
    "        pdf_path = fetch_paper_pdf(paper['id'], output_dir=\"./papers\")\n",
    "        if pdf_path:\n",
    "            print(f\"   âœ“ Saved to: {pdf_path}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"ðŸ“‹ Papers ready for download (set DOWNLOAD_PDFS = True to fetch):\")\n",
    "    for i, paper in enumerate(high_value_papers[:3], 1):\n",
    "        print(f\"   {i}. {paper['title'][:60]}...\")\n",
    "        print(f\"      URL: https://arxiv.org/pdf/{paper['id'].replace('v1', '')}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: From Batch Processing to Intelligent Agents\n",
    "\n",
    "We've evolved our system through several stages:\n",
    "\n",
    "| Stage | Approach | Limitation |\n",
    "|-------|----------|------------|\n",
    "| **Part 1** | LLM batch evaluation | Scores cluster around 7-8, hard to prioritize |\n",
    "| **Part 2** | Add citation signals | Now have two independent metrics |\n",
    "| **Part 3** | Composite scoring | Combine relevance + impact for better ranking |\n",
    "| **Part 4** | Tool orchestration | Agent decides WHEN to use expensive tools |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LLMs are \"polite\"** - They tend to give middle-ground scores. External signals add variance.\n",
    "\n",
    "2. **Multiple signals > single signal** - Combining LLM judgment with community metrics gives better rankings.\n",
    "\n",
    "3. **Agents make decisions** - Instead of running every tool on every input, agents reason about when tools are worth using.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next part, we'll give the LLM the ability to:\n",
    "- Choose which tools to call autonomously\n",
    "- Decide what to do based on results  \n",
    "- Iterate until the task is complete\n",
    "- Handle errors and recover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers That Didn't Make the Cut\n",
    "\n",
    "Let's also look at low-scoring papers to understand the AI's reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Low-Scoring Papers (why weren't they relevant?):\n",
      "================================================================================\n",
      "\n",
      "[6/10] Safety Alignment in Large Language Models: Methods and Chall...\n",
      "   Why: Background knowledge but not directly applicable to current work.\n",
      "\n",
      "[5/10] Scaling Laws for Fine-tuning Language Models...\n",
      "   Why: Useful but focused on training dynamics rather than inference optimization.\n",
      "\n",
      "[5/10] Advanced Topics in Transformer Architecture Design...\n",
      "   Why: More focused on architecture design than practical application.\n",
      "\n",
      "[4/10] Theory of Mind in Large Language Models: An Empirical Invest...\n",
      "   Why: Interesting but more theoretical than practical for the team.\n"
     ]
    }
   ],
   "source": [
    "# Show papers with low scores\n",
    "print(\"ðŸ“‰ Low-Scoring Papers (why weren't they relevant?):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "low_scoring = [p for p in results if p.get('relevance_score', 0) < 7]\n",
    "\n",
    "for paper in low_scoring[:5]:\n",
    "    score = paper.get('relevance_score', 0)\n",
    "    print(f\"\\n[{score}/10] {paper['title'][:60]}...\")\n",
    "    print(f\"   Why: {paper.get('why_relevant', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: What Worked / What Didn't\n",
    "\n",
    "**What worked well:**\n",
    "- âœ… Papers about practical topics (agents, inference, deployment) scored high\n",
    "- âœ… The AI correctly identified papers matching our \"interests\" list\n",
    "- âœ… Papers flagged in \"avoid\" (pure theory, training-focused) scored lower\n",
    "\n",
    "**Limitations of this approach:**\n",
    "- âŒ **No iteration** - we can't ask follow-up questions\n",
    "- âŒ **No context** - each paper is evaluated independently\n",
    "- âŒ **No actions** - can't download PDFs, read full text, etc.\n",
    "- âŒ **Fixed criteria** - can't adjust mid-evaluation\n",
    "\n",
    "**This is why we need agents!**\n",
    "\n",
    "In Part 2, we'll give the AI tools to:\n",
    "- Search for related papers\n",
    "- Download and read full PDFs\n",
    "- Summarize findings\n",
    "- Decide what to do next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Try these modifications to better understand the system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Change the Team Profile\n",
    "\n",
    "Modify `TEAM_PROFILE` above to match a different team's interests.\n",
    "For example, a team focused on healthcare AI or computer vision.\n",
    "Then re-run the evaluation to see how scores change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Healthcare AI team profile\n",
    "HEALTHCARE_TEAM_PROFILE = {\n",
    "    \"focus\": \"\"\"\n",
    "        The team builds AI systems for healthcare applications including\n",
    "        medical imaging analysis, clinical decision support, and patient\n",
    "        communication tools.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"interests\": [\n",
    "        \"Medical image analysis and diagnosis\",\n",
    "        \"Clinical NLP and medical text understanding\",\n",
    "        \"Privacy-preserving machine learning\",\n",
    "        \"Explainable AI for healthcare\",\n",
    "        \"Patient-facing AI applications\",\n",
    "    ],\n",
    "    \n",
    "    \"avoid\": [\n",
    "        \"General-purpose language models without medical focus\",\n",
    "        \"Papers without clinical validation\",\n",
    "        \"Purely theoretical work\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Try using this profile instead!\n",
    "# Uncomment the line below and re-run the batch evaluation\n",
    "# TEAM_PROFILE = HEALTHCARE_TEAM_PROFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adjust the Scoring Criteria\n",
    "\n",
    "Modify `create_evaluation_prompt()` to use different scoring criteria.\n",
    "For example, add factors like \"novelty\" or \"implementation difficulty\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fetch Live Papers\n",
    "\n",
    "Set `USE_LIVE_DATA = True` and `RUN_BATCH_API = True` to:\n",
    "1. Fetch real papers from arXiv\n",
    "2. Evaluate them with the actual Batch API\n",
    "\n",
    "Compare the results with the sample data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In Part 1, we built a **baseline** paper evaluation system:\n",
    "\n",
    "1. **Fetch** - Retrieved 20 papers from arXiv\n",
    "2. **Batch** - Created JSONL file with evaluation requests\n",
    "3. **Submit** - Sent to the Batch API\n",
    "4. **Parse** - Extracted structured JSON from responses\n",
    "5. **Rank** - Sorted papers by relevance score\n",
    "\n",
    "**Key insight:** This is a fixed pipeline. The AI evaluates papers but can't:\n",
    "- Ask clarifying questions\n",
    "- Decide to search for more papers\n",
    "- Read full paper text\n",
    "- Take any actions beyond scoring\n",
    "\n",
    "**Next up in Part 2:** We'll add tools and turn this into a true AI agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the batch file\n",
    "import os\n",
    "if os.path.exists('batch_input.jsonl'):\n",
    "    os.remove('batch_input.jsonl')\n",
    "    print(\"âœ“ Cleaned up batch_input.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}