{"custom_id": "2602.02495v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Reward-free Alignment for Conflicting Objectives\n\nAbstract:\nDirect alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02477v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability\n\nAbstract:\nLarge language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02474v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents\n\nAbstract:\nMost Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02470v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge\n\nAbstract:\nAutoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02468v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts\n\nAbstract:\nDespite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02467v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models\n\nAbstract:\nRapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02465v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: MentisOculi: Revealing the Limits of Reasoning with Mental Imagery\n\nAbstract:\nFrontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02462v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models\n\nAbstract:\nLarge Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02457v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: MetaCLASS: Metacognitive Coaching for Learning with Adaptive Self-regulation Support\n\nAbstract:\nLarge language models can generate fluent explanations, but effective tutoring requires supporting the learner's thought process, not just delivering content. Metacognitive tutoring targets this gap by prompting planning, monitoring, debugging, and evaluation, and crucially, deciding when to be active versus minimally present, based on learner signals and trajectory. We introduce MetaCLASS, a learning-science grounded framework that formulates metacognitive tutoring as move selection over 11 interpretable actions aligned to self-regulated learning processes. MetaCLASS uses a two-phase framework that first plans a pedagogical trajectory conditioned on learner profiles (calibration, help-seeking) and then generates natural dialogue consistent with that plan. This yields a dataset of 1,015 conversations (7,711 turns) annotated with turn-level metacognitive labels, and validated for pedagogical contingency and trajectory adherence. We benchmark nine LLMs on predicting the next coach move given the problem and dialogue context. The best model achieves only 43.2\\% accuracy, and models exhibit compulsive intervention bias: in turns where effective metacognitive tutoring requires silent (41.7\\% of cases), models predict `no intervention' only 4.2\\% of the time, while severely over-predicting high-intervention moves. These results show that traditional content-based tutoring ability does not translate to metacognitive tutoring competence, positioning MetaCLASS as a testbed for developing intelligent tutors that promote self-regulated learning.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02456v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning\n\nAbstract:\nRepresenting and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02455v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction\n\nAbstract:\nAs Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02453v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling\n\nAbstract:\nChain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02440v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Large Language Models for Mental Health: A Multilingual Evaluation\n\nAbstract:\nLarge Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02427v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning\n\nAbstract:\nLarge language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02414v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank\n\nAbstract:\nTimely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02405v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning\n\nAbstract:\nImproving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02400v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence\n\nAbstract:\nLarge-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02395v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning\n\nAbstract:\nThe evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02386v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing\n\nAbstract:\nHow should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
{"custom_id": "2602.02383v1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8", "max_tokens": 500, "messages": [{"role": "user", "content": "You are evaluating research papers for an AI engineering team.\n\nTEAM PROFILE:\nThe team is building AI-powered applications and wants to stay\n        current with the latest research on language models, inference\n        optimization, and practical AI engineering.\n\nWhat they find valuable:\n  - Large language model architectures and improvements\n  - Inference optimization and cost reduction\n  - Prompt engineering and techniques\n  - AI agents and tool use\n  - Evaluation methods for LLMs\n\nWhat to avoid recommending:\n  - Pure theoretical papers without practical applications\n  - Incremental benchmark improvements\n  - Papers focused only on training from scratch\n  - Papers focsed on training models\n\n---\n\nPAPER TO EVALUATE:\n\nTitle: SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization\n\nAbstract:\nDirect preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.\n\n---\n\nINSTRUCTIONS:\nScore this paper's relevance to the team on a scale of 0-10.\n- 0-3: Not relevant\n- 4-6: Somewhat relevant\n- 7-10: Highly relevant, team should read this\n\nRespond with ONLY valid JSON in this exact format:\n{\n    \"relevance_score\": <integer 0-10>,\n    \"key_insight\": \"<one sentence explaining the main takeaway>\",\n    \"why_relevant\": \"<one sentence explaining why this score>\"\n}"}]}}
